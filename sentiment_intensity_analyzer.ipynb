{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import html\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df_comments1 = pd.read_pickle('./pickle_dataframes/comments1.pkl')\n",
    "df_comments2 = pd.read_pickle('./pickle_dataframes/comments2.pkl')\n",
    "df_comments = pd.concat([df_comments1,df_comments2])\n",
    "df_comments.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_posts1 = pd.read_pickle('./pickle_dataframes/posts1.pkl')\n",
    "df_posts2 = pd.read_pickle('./pickle_dataframes/posts2.pkl')\n",
    "df_posts3 = pd.read_pickle('./pickle_dataframes/posts3.pkl')\n",
    "df_posts = pd.concat([df_posts1, df_posts2, df_posts3])\n",
    "df_posts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_postlinks = pd.read_pickle('./pickle_dataframes/posts_links.pkl')\n",
    "df_tags = pd.read_pickle('./pickle_dataframes/tags.pkl')\n",
    "df_users = pd.read_pickle('./pickle_dataframes/users.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally filter only active users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify preprocess_text function\n",
    "def preprocess_text(text, remove_stopwords=False, use_lemmatize=True, use_stemmer=False):\n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text.lower())\n",
    "\n",
    "    words = text.split()\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "    if use_lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    elif use_stemmer:  # Apply stemming only if use_stemmer is True\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Title, Body, and Tags using Dask DataFrames\n",
    "# Convert the DFs to Dask DataFrames\n",
    "ddf_comments = dd.from_pandas(df_comments, npartitions=8)  # adjust the number of partitions as needed\n",
    "ddf_posts = dd.from_pandas(df_posts, npartitions=8)  # adjust the number of partitions as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Columns\n",
    "ddf_comments['Text'] = ddf_comments['Text'].map_partitions(lambda x: x.apply(lambda y: preprocess_text(y, remove_stopwords=True, use_lemmatize=True, use_stemmer=False)))\n",
    "ddf_comments.to_pickle('df_comments_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Posts\n",
    "ddf_posts['Title'] = ddf_posts['Title'].map_partitions(lambda x: x.apply(lambda y: preprocess_text(y, remove_stopwords=True, use_lemmatize=True, use_stemmer=False)))\n",
    "ddf_posts['Body'] = ddf_posts['Body'].map_partitions(lambda x: x.apply(lambda y: preprocess_text(y, remove_stopwords=True, use_lemmatize=True, use_stemmer=False)))\n",
    "ddf_posts.to_pickle('df_posts_processed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SentimentIntensityAnalyzer once\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to apply sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    # Check if the text is missing or NaN, return 0.0 in such cases\n",
    "    if pd.isna(text):\n",
    "        return 0.0\n",
    "    # Ensure the text is encoded as a string\n",
    "    text = str(text)\n",
    "    return sia.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_data(df, npartitions=None):\n",
    "#     start_time = time.time()\n",
    "#     \n",
    "#     # If npartitions is not specified, default to 1 (suitable for sequential processing)\n",
    "#     if npartitions is None:\n",
    "#         npartitions = 1\n",
    "# \n",
    "#     # Convert DataFrame to Dask DataFrame\n",
    "#     ddf = dd.from_pandas(df, npartitions=npartitions)\n",
    "#     # Apply sentiment analysis\n",
    "#     ddf['sentiment'] = ddf['Body'].map(analyze_sentiment, meta=('Body', 'float64'))\n",
    "# \n",
    "#     # Compute result and monitor memory usage\n",
    "#     result = ddf.compute()\n",
    "#     memory_usage = psutil.virtual_memory()\n",
    "#     \n",
    "#     end_time = time.time()\n",
    "#     return result, end_time - start_time, memory_usage.used\n",
    "\n",
    "# Sequential processing (no parallelism)\n",
    "# print(\"Running sequentially...\")\n",
    "# seq_result, seq_time, _ = process_data(df_posts)\n",
    "# print(f\"Sequential processing time: {seq_time} seconds\")\n",
    "# \n",
    "# # Parallel processing with multiple cores\n",
    "# core_counts = [2, 4, 6, 7, 8, 10]\n",
    "# for cores in core_counts:\n",
    "#     print(f\"Running with {cores} cores...\")\n",
    "#     with Client(n_workers=cores, threads_per_worker=2) as client:  # Adjust threads_per_worker as needed\n",
    "#         _, parallel_time, mem_usage = process_data(df_posts, npartitions=cores)\n",
    "#         efficiency = seq_time / (cores * parallel_time)\n",
    "#         print(f\"Time with {cores} cores: {parallel_time} seconds, Efficiency: {efficiency}, Memory used: {mem_usage} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 43.63 ss\n",
      "[########################################] | 100% Completed | 12.50 ss\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrame to Dask DataFrame\n",
    "df_comments_dask = dd.from_pandas(df_comments, npartitions=8)  # Adjust npartitions based on memory usage results\n",
    "df_posts_dask = dd.from_pandas(df_posts, npartitions=8)  # Adjust npartitions based on memory usage results\n",
    "\n",
    "# Apply sentiment analysis\n",
    "df_comments_dask['sentiment'] = df_comments_dask['Text'].map(analyze_sentiment)\n",
    "df_posts_dask['body_sentiment'] = df_posts_dask['Body'].map(analyze_sentiment)\n",
    "df_posts_dask['title_sentiment'] = df_posts_dask['Title'].map(analyze_sentiment)\n",
    "\n",
    "# Compute the results with progress bar\n",
    "with ProgressBar():\n",
    "    df_comments_result = ddf_comments.compute()\n",
    "    df_posts_result = ddf_posts.compute()\n",
    "\n",
    "df_comments_result.to_pickle('df_comments_result.pkl')\n",
    "df_posts_result.to_pickle('df_posts_result.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
