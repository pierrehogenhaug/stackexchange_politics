{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df_comments1 = pd.read_pickle('./pickle_dataframes/comments1.pkl')\n",
    "df_comments2 = pd.read_pickle('./pickle_dataframes/comments2.pkl')\n",
    "df_comments = pd.concat([df_comments1,df_comments2])\n",
    "df_comments.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_posts1 = pd.read_pickle('./pickle_dataframes/posts1.pkl')\n",
    "df_posts2 = pd.read_pickle('./pickle_dataframes/posts2.pkl')\n",
    "df_posts3 = pd.read_pickle('./pickle_dataframes/posts3.pkl')\n",
    "df_posts = pd.concat([df_posts1, df_posts2, df_posts3])\n",
    "df_posts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_postlinks = pd.read_pickle('./pickle_dataframes/posts_links.pkl')\n",
    "df_tags = pd.read_pickle('./pickle_dataframes/tags.pkl')\n",
    "df_users = pd.read_pickle('./pickle_dataframes/users.pkl')\n",
    "\n",
    "df_posts = df_posts[df_posts['PostTypeId'] == 1]\n",
    "#df_posts = df_posts.sample(frac=0.25)\n",
    "\n",
    "# Remove entries with -1 in UserId and OwnerUserId columns\n",
    "df_comments = df_comments[df_comments['UserId'] != -1]\n",
    "df_posts = df_posts[df_posts['OwnerUserId'] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SentimentIntensityAnalyzer once\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to apply sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    return sia.polarity_scores(text)['compound']\n",
    "\n",
    "def process_data(df, npartitions=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # If npartitions is not specified, default to 1 (suitable for sequential processing)\n",
    "    if npartitions is None:\n",
    "        npartitions = 1\n",
    "\n",
    "    # Convert DataFrame to Dask DataFrame\n",
    "    ddf = dd.from_pandas(df, npartitions=npartitions)\n",
    "    # Apply sentiment analysis\n",
    "    ddf['sentiment'] = ddf['Body'].map(analyze_sentiment, meta=('Body', 'float64'))\n",
    "\n",
    "    # Compute result and monitor memory usage\n",
    "    result = ddf.compute()\n",
    "    memory_usage = psutil.virtual_memory()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time, memory_usage.used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sequentially...\n",
      "Sequential processing time: 23.122904062271118 seconds\n",
      "Running with 2 cores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pih\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\distributed\\client.py:3163: UserWarning: Sending large graph of size 19.11 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with 2 cores: 15.003249406814575 seconds, Efficiency: 0.770596536633209, Memory used: 13127147520 bytes\n",
      "Running with 4 cores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pih\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\distributed\\client.py:3163: UserWarning: Sending large graph of size 19.11 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with 4 cores: 9.33157753944397 seconds, Efficiency: 0.6194800387322538, Memory used: 13367934976 bytes\n",
      "Running with 6 cores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pih\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\distributed\\client.py:3163: UserWarning: Sending large graph of size 19.11 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with 6 cores: 8.02925729751587 seconds, Efficiency: 0.4799718331238639, Memory used: 13698764800 bytes\n",
      "Running with 7 cores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pih\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\distributed\\client.py:3163: UserWarning: Sending large graph of size 19.11 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with 7 cores: 7.701604604721069 seconds, Efficiency: 0.42890698476924854, Memory used: 13861740544 bytes\n",
      "Running with 8 cores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pih\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\distributed\\client.py:3163: UserWarning: Sending large graph of size 19.11 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with 8 cores: 7.600461483001709 seconds, Efficiency: 0.3802878304492606, Memory used: 14376480768 bytes\n",
      "Running with 10 cores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pih\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\distributed\\client.py:3163: UserWarning: Sending large graph of size 19.11 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with 10 cores: 7.4803972244262695 seconds, Efficiency: 0.30911331803030817, Memory used: 14571917312 bytes\n"
     ]
    }
   ],
   "source": [
    "# Sequential processing (no parallelism)\n",
    "print(\"Running sequentially...\")\n",
    "seq_result, seq_time, _ = process_data(df_posts)\n",
    "print(f\"Sequential processing time: {seq_time} seconds\")\n",
    "\n",
    "# Parallel processing with multiple cores\n",
    "core_counts = [2, 4, 6, 7, 8, 10]\n",
    "for cores in core_counts:\n",
    "    print(f\"Running with {cores} cores...\")\n",
    "    with Client(n_workers=cores, threads_per_worker=2) as client:  # Adjust threads_per_worker as needed\n",
    "        _, parallel_time, mem_usage = process_data(df_posts, npartitions=cores)\n",
    "        efficiency = seq_time / (cores * parallel_time)\n",
    "        print(f\"Time with {cores} cores: {parallel_time} seconds, Efficiency: {efficiency}, Memory used: {mem_usage} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrame to Dask DataFrame\n",
    "df_comments_dask = dd.from_pandas(df_comments, npartitions=10)  # Adjust npartitions as needed\n",
    "df_posts_dask = dd.from_pandas(df_posts, npartitions=10)  # Adjust npartitions as needed\n",
    "\n",
    "# Apply sentiment analysis\n",
    "df_comments_dask['sentiment'] = df_comments_dask['Text'].map(analyze_sentiment)\n",
    "df_posts_dask['body_sentiment'] = df_posts_dask['Body'].map(analyze_sentiment)\n",
    "df_posts_dask['title_sentiment'] = df_posts_dask['Title'].map(analyze_sentiment)\n",
    "\n",
    "# Compute the results with progress bar\n",
    "with ProgressBar():\n",
    "    df_comments_result = df_comments_dask.compute()\n",
    "    df_posts_result = df_posts_dask.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
