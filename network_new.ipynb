{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import community as community_louvain\n",
    "import dask.dataframe as dd\n",
    "import html\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import re\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Generation [1:2]: The DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "questions = pd.read_pickle('./pickle_dataframes/questions_with_sentiment.pkl')\n",
    "answers = pd.concat([pd.read_pickle('./pickle_dataframes/answers_with_sentiment1.pkl'), \n",
    "                     pd.read_pickle('./pickle_dataframes/answers_with_sentiment2.pkl')]).reset_index(drop=True)\n",
    "comments = pd.read_pickle('./pickle_dataframes/comments_with_sentiment.pkl')\n",
    "\n",
    "users = pd.read_pickle('./pickle_dataframes/users_with_all_attributes.pkl')\n",
    "\n",
    "users_with_all_attributes = pd.read_pickle('./pickle_dataframes/users_with_all_attributes.pkl')\n",
    "users_with_all_attributes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing a list of question IDs\n",
    "parent_list = questions.Id.tolist()\n",
    "\n",
    "# Identifying Comments Associated with Questions and Answers\n",
    "comments_on_questions = comments[comments['PostId'].isin(questions['Id'])]\n",
    "comments_on_answers = comments[comments['PostId'].isin(answers['Id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating commenters by the post they commented on\n",
    "comments_on_questions_agg = comments_on_questions.groupby('PostId')['UserId'].apply(list).reset_index()\n",
    "comments_on_answers_agg = comments_on_answers.groupby('PostId')['UserId'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Answer IDs to their corresponding Question IDs\n",
    "answer_to_question_map = answers.set_index('Id')['ParentId'].to_dict()\n",
    "comments_on_answers_agg['MappedPostId'] = comments_on_answers_agg['PostId'].map(lambda x: answer_to_question_map.get(x, None))\n",
    "\n",
    "# Filtering out None values which have no corresponding question\n",
    "comments_on_answers_agg = comments_on_answers_agg[comments_on_answers_agg['MappedPostId'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a unified DataFrame for comment data\n",
    "comments_combined = pd.concat([\n",
    "    comments_on_questions_agg.rename(columns={'PostId': 'QuestionId', 'UserId': 'CommentOnQuestionUserId_list'}),\n",
    "    comments_on_answers_agg.rename(columns={'MappedPostId': 'QuestionId', 'UserId': 'CommentOnAnswersUserId_list'})\n",
    "], ignore_index=True)\n",
    "\n",
    "# Replacing NaN values with empty lists\n",
    "comments_combined['CommentOnQuestionUserId_list'] = comments_combined['CommentOnQuestionUserId_list'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "comments_combined['CommentOnAnswersUserId_list'] = comments_combined['CommentOnAnswersUserId_list'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# Grouping and combining lists\n",
    "comments_combined = comments_combined.groupby('QuestionId').agg(\n",
    "    CommentOnQuestionUserId_list=('CommentOnQuestionUserId_list', lambda x: sum(x, [])),\n",
    "    CommentOnAnswersUserId_list=('CommentOnAnswersUserId_list', lambda x: sum(x, []))\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering answers that are related to the collected questions\n",
    "df_int = answers[answers.ParentId.isin(parent_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping answers by their parent question and aggregating answerer user IDs\n",
    "df_subpost = df_int.groupby('ParentId').agg(\n",
    "    answers_UserId_list=('OwnerUserId', lambda x: list(x))\n",
    ").reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying original posters for each question\n",
    "df_original_poster = questions[questions.Id.isin(df_int.ParentId.tolist())].copy()\n",
    "df_original_poster = df_original_poster.groupby('Id').agg(\n",
    "    original_poster_UserId=('OwnerUserId', lambda x: list(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging to form a comprehensive DataFrame for graph construction\n",
    "df_graph = pd.merge(\n",
    "    left=df_original_poster,\n",
    "    right=df_subpost,\n",
    "    left_on='Id',\n",
    "    right_on='ParentId'\n",
    ")\n",
    "\n",
    "# Cleaning up the 'original_poster' column\n",
    "df_graph['original_poster_UserId'] = df_graph['original_poster_UserId'].apply(lambda x: x[0] if x else None)\n",
    "\n",
    "# Integrating Comment Data with the Graph Data\n",
    "df_graph = pd.merge(df_graph, comments_combined, left_on='ParentId', right_on='QuestionId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph['CommentOnQuestionUserId_list'] = df_graph['CommentOnQuestionUserId_list'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "df_graph['CommentOnAnswersUserId_list'] = df_graph['CommentOnAnswersUserId_list'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "df_graph = df_graph[df_graph['original_poster_UserId'] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you want to see how to get each of the values in df_graph's columns expand below rows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the different columns of row 1 manually\n",
    "df_graph.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at question 1\n",
    "# We see that it belongs to questions['OwnerUserId']==18 (original_poster_UserId==18)\n",
    "questions[questions['Id']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at the answers to question 1\n",
    "# We see that the answers['OwnerUserId'] corresponds to the users in df_graph['answers_UserId_list'] == [26, 8, 4666]\t\n",
    "answers[answers['ParentId']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at the comments on the questions (CommentersOnQuestion)\n",
    "# We see that the comments['UserId'] corresponds to the users in df_graph['CommentersOnQuestion'] == [28, 18, 8018]\t\n",
    "comments[comments['PostId']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at the comments on the first/3 of the question answers\n",
    "# We see that the comments['UserId'] corresponds to one of the user in df_graph['CommentersOnAnswers'] == [7014, 9921]\t \n",
    "comments[comments['PostId']==4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Generation [2:2]: The graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have to consider how we connect** \n",
    "- original_poster_UserId to CommentOnAnswersUserId_list\n",
    "- answers_UserId_list to CommentOnAnswersUserId_list\n",
    "\n",
    "Do we connect both or only one of them?\n",
    "- In `G1` we connect original_poster_UserId to CommentOnAnswersUserId_list\n",
    "\n",
    "In a more extensive study, we would've also defined `G2` and `G3`:\n",
    "- In `G2` we connect answers_UserId_list to CommentOnAnswersUserId_list\n",
    "- In `G3` we connect both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1 = nx.Graph()\n",
    "# - original_poster_UserId to CommentOnAnswersUserId_list\n",
    "\n",
    "# Adding nodes and edges for original posters, answerers, and commenters\n",
    "for idx, row in df_graph.iterrows():\n",
    "    original_poster_UserId = row['original_poster_UserId']\n",
    "    G1.add_node(original_poster_UserId)\n",
    "\n",
    "    # Add edges from original poster to answerers\n",
    "    for user in row['answers_UserId_list']:\n",
    "        G1.add_edge(original_poster_UserId, user)\n",
    "\n",
    "    # Add edges from original poster to commenters on the question\n",
    "    if isinstance(row['CommentOnQuestionUserId_list'], list):\n",
    "        for commenter in row['CommentOnQuestionUserId_list']:\n",
    "            G1.add_edge(original_poster_UserId, commenter)\n",
    "\n",
    "    # Add edges from original poster to commenters on the answers\n",
    "    if isinstance(row['CommentOnAnswersUserId_list'], list):\n",
    "        for commenter in row['CommentOnAnswersUserId_list']:\n",
    "            G1.add_edge(original_poster_UserId, commenter)\n",
    "\n",
    "print(f'len(G1.nodes(): {len(G1.nodes(data=True))}')\n",
    "print(f'len(G1.edges()): {len(G1.edges())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection: Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate different network statistics\n",
    "avg_degree_centrality = sum(nx.degree_centrality(G1).values()) / len(G1)\n",
    "avg_clustering_coefficient = nx.average_clustering(G1)\n",
    "connected_components = list(nx.connected_components(G1))\n",
    "avg_betweenness_centrality = sum(nx.betweenness_centrality(G1).values()) / len(G1)\n",
    "communities = greedy_modularity_communities(G1)\n",
    "\n",
    "# Analysis - You might print these or plot them using matplotlib or similar\n",
    "print(\"Average Degree Centrality:\", avg_degree_centrality)\n",
    "print(\"Average Clustering Coefficient:\", avg_clustering_coefficient)\n",
    "print(\"Number of Connected Components:\", len(connected_components))\n",
    "print(\"Average Betweenness Centrality:\", avg_betweenness_centrality)\n",
    "print(\"Number of Communities detected:\", len(communities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect communities\n",
    "partition = community_louvain.best_partition(G1)\n",
    "\n",
    "# Assign community label to each node\n",
    "for node, community in partition.items():\n",
    "    G1.nodes[node]['community'] = community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1.nodes(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Node data to dataframe\n",
    "node_data = [{'UserId': node, 'Community': data['community']} for node, data in G1.nodes(data=True)]\n",
    "community_df = pd.DataFrame(node_data)\n",
    "community_df.Community.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection: K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cluster(df, clustering_features, n_clusters, clustering_name, random_state=42, verbose=False, minmax=True):\n",
    "    # Impute NaNs and scale features\n",
    "    features = df[clustering_features].fillna(0)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    # KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "    df[clustering_name] = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "    # Display plots\n",
    "    if verbose:\n",
    "        # Elbow method and silhouette analysis\n",
    "        wcss, silhouette_scores = [], []\n",
    "        for i in range(1, 11):\n",
    "            kmeans_i = KMeans(n_clusters=i, random_state=random_state).fit(scaled_features)\n",
    "            wcss.append(kmeans_i.inertia_)\n",
    "            if i > 1:\n",
    "                score = silhouette_score(scaled_features, kmeans_i.labels_)\n",
    "                silhouette_scores.append(score)\n",
    "\n",
    "        # Plot elbow method and silhouette analysis results\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axs[0].plot(range(1, 11), wcss)\n",
    "        axs[0].set_title('Elbow Method')\n",
    "        axs[0].set_xlabel('Number of Clusters')\n",
    "        axs[0].set_ylabel('WCSS')\n",
    "        axs[0].grid(True)\n",
    "\n",
    "        axs[1].plot(range(2, 11), silhouette_scores)\n",
    "        axs[1].set_title('Silhouette Analysis')\n",
    "        axs[1].set_xlabel('Number of Clusters')\n",
    "        axs[1].set_ylabel('Silhouette Score')\n",
    "        axs[1].grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot boxplots of feature distributions\n",
    "        num_features = len(clustering_features)\n",
    "        num_rows = (num_features - 1) // 5 + 1\n",
    "        num_cols = min(num_features, 5)\n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize=(4*num_cols, 4*num_rows))\n",
    "        axs = axs.flatten() if num_features > 1 else [axs]\n",
    "        for i, feature in enumerate(clustering_features):\n",
    "            sns.boxplot(x=clustering_name, y=feature, data=df, ax=axs[i])\n",
    "            axs[i].set(title=f'{feature} Distribution', xlabel='Cluster', ylabel='Count')\n",
    "        for ax in axs[len(clustering_features):]:\n",
    "            ax.set_visible(False)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Compute cluster information\n",
    "    cluster_counts = df[clustering_name].value_counts().sort_index().reset_index(name='Count')\n",
    "    cluster_means = df.groupby(clustering_name)[clustering_features].mean().add_suffix('Mean')\n",
    "    cluster_info = pd.merge(cluster_counts, cluster_means, on=clustering_name)\n",
    "\n",
    "    # Plot lineplot of cluster information\n",
    "    if verbose:\n",
    "        if minmax:  # MinMax scaling for lineplot\n",
    "            minmax_scaler = MinMaxScaler()\n",
    "            mean_feature_names = [f'{feature}Mean' for feature in clustering_features]\n",
    "            scaled = cluster_info[mean_feature_names].apply(lambda x: minmax_scaler.fit_transform(x.values.reshape(-1, 1)).flatten())\n",
    "            scaled[clustering_name] = cluster_info[clustering_name]\n",
    "            melted = scaled.melt(id_vars=[clustering_name], var_name='Feature', value_name='NormalizedValue')\n",
    "            ylabel = \"Per-Feature Normalised Value\"\n",
    "        else:  # Absolute values for lineplot\n",
    "            melted = cluster_info.drop('Count', axis=1).melt(id_vars=[clustering_name], var_name='Feature', value_name='Mean')\n",
    "            ylabel = \"Mean Value\"\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        value_column = 'NormalizedValue' if minmax else 'Mean'\n",
    "        title = \"Per-Feature Normalised Values Across Clusters\" if minmax else \"Mean Values of Features Across Clusters\"\n",
    "        for cluster in melted[clustering_name].unique():\n",
    "            data = melted[melted[clustering_name] == cluster]\n",
    "            plt.plot(data['Feature'], data[value_column], label=f'Cluster {cluster}')\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Feature\")\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return cluster_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test_clustering(df, clustering_name, clustering_features, num_permutations=1000, verbose=False):\n",
    "    # Impute NaNs and scale features\n",
    "    features = df[clustering_features].fillna(0)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    # Compute silhouette score using actual labels\n",
    "    actual_labels = df[clustering_name].values\n",
    "    actual_silhouette_score = silhouette_score(scaled_features, actual_labels)\n",
    "    \n",
    "    # Compute silhouette scores using permuted labels\n",
    "    permuted_silhouette_scores = []\n",
    "    for _ in range(num_permutations):\n",
    "        shuffled_labels = np.random.permutation(actual_labels)\n",
    "        permuted_silhouette_score = silhouette_score(scaled_features, shuffled_labels)\n",
    "        permuted_silhouette_scores.append(permuted_silhouette_score)\n",
    "\n",
    "    # Calculate p-value for significance testing\n",
    "    p_value = (np.sum(np.array(permuted_silhouette_scores) >= actual_silhouette_score) + 1) / (num_permutations + 1)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Actual silhouette score: {actual_silhouette_score}')\n",
    "        print(f'p-value: {p_value}')\n",
    "\n",
    "    return actual_silhouette_score, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clustering by Activity Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Clustering by question count, answer count, and comment count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_features = ['QuestionCount', 'AnswerCount', 'CommentCount']\n",
    "clustering_name = 'ActivityCluster'\n",
    "\n",
    "custom_cluster(users_with_all_attributes, clustering_features, 4, clustering_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_test_clustering(users_with_all_attributes, clustering_name, clustering_features, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clustering by Quality Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Clustering by reputation, average question score, average answer score, and average comment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_features = ['Reputation', 'AvgQuestionScore', 'AvgAnswerScore', 'AvgCommentScore']\n",
    "clustering_name = 'QualityCluster1'\n",
    "\n",
    "custom_cluster(users_with_all_attributes, clustering_features, 4, clustering_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_test_clustering(users_with_all_attributes, clustering_name, clustering_features, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Clustering by accepted answer count and accepted answer fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_features = ['AcceptedAnswerCount', 'AcceptedAnswerFraction']\n",
    "clustering_name = 'QualityCluster2'\n",
    "\n",
    "custom_cluster(users_with_all_attributes, clustering_features, 3, clustering_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_test_clustering(users_with_all_attributes, clustering_name, clustering_features, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clustering by Sentiment Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Clustering by average question body sentiment, average question title sentiment, average answer sentiment, average comment sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_features = ['AvgQuestionBodySentiment', 'AvgQuestionTitleSentiment', 'AvgAnswerSentiment', 'AvgCommentSentiment']\n",
    "clustering_name = 'SentimentCluster'\n",
    "\n",
    "custom_cluster(users_with_all_attributes, clustering_features, 4, clustering_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_test_clustering(users_with_all_attributes, clustering_name, clustering_features, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clustering by Engagement Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Clustering by views, upvotes, and downvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_features = ['Views', 'UpVotes', 'DownVotes']\n",
    "clustering_name = 'EngagementCluster'\n",
    "\n",
    "custom_cluster(users_with_all_attributes, clustering_features, 3, clustering_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_test_clustering(users_with_all_attributes, clustering_name, clustering_features, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clustering by Topic Engagement Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Clustering by topic engagement columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_features = list(range(25))\n",
    "clustering_name = 'TopicEngagementCluster'\n",
    "\n",
    "custom_cluster(users_with_all_attributes, clustering_features, 7, clustering_name, verbose=True, minmax=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_test_clustering(users_with_all_attributes, clustering_name, clustering_features, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning Node Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_all_attributes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserAttributesForNetwork = users_with_all_attributes[['Id', 'MostEngagedTopic', 'EngagementCluster', 'ActivityCluster', 'Quality1Cluster', 'Quality2Cluster', 'SentimentCluster', 'EngagementSentimentCluster', 'Engagement_MostEngaged_Cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_attrs = UserAttributesForNetwork.set_index('Id').T.to_dict()\n",
    "list(user_attrs.items())[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the user attributes dictionary and add each to the corresponding node\n",
    "for user_id, attrs in user_attrs.items():\n",
    "    if user_id in G1.nodes:\n",
    "        nx.set_node_attributes(G1, {user_id: attrs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check attributes of a specific user node\n",
    "print(G1.nodes[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Network to Gephi\n",
    "# nx.write_graphml(G1, './graphml/graph2.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusters vs. Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Attribute Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Reshuffleing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *\"The \"label shuffling technique\" is incredibly useful. It may turn out to be a good tool to apply for your independent project. Keep it in mind.\"* - **Week8.ipynb**\n",
    "\n",
    "Use the \"label shuffling test\" (Week 5 and 8) to test if the coast with the highest wikipedia page sentiment has a page sentiment that is significantly higher (5% confidence bound) than a randomly selected group of rappers of the same size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initial Cluster Analysis\n",
    "Cluster Your Data: Use a clustering algorithm (like k-means) to cluster your users based on the features you're interested in. Assign each user to a cluster.\n",
    "Compute Initial Statistics: For each cluster, compute the average (or another statistic of interest) of a specific feature or set of features. These are your observed values.\n",
    "2. Permutation Test Procedure\n",
    "Shuffle Cluster Labels: Randomly shuffle the cluster labels assigned to the users, ensuring that the number of users in each cluster remains the same as in the original classification.\n",
    "Recompute Statistics for Shuffled Data: For each shuffled configuration, recompute the same statistics as in your initial analysis for each cluster.\n",
    "Repeat the Process: Perform this shuffling and recomputing process a large number of times (typically 1000 or more) to build a distribution of the statistic under the null hypothesis.\n",
    "3. Analysis and Comparison\n",
    "Create Histograms: For each cluster, create histograms of the computed statistics from the shuffled data.\n",
    "Compare Observed Values with Distributions: Compare the initially observed values for each cluster with the distributions obtained from the shuffled data. If your observed value lies outside the bulk of the distribution for shuffled data, it suggests that the observed value is not simply due to random chance.\n",
    "4. Statistical Significance\n",
    "P-Value Calculation: For each cluster, you can calculate a p-value, which is the proportion of the shuffled datasets where the computed statistic was as extreme as the observed statistic. A small p-value indicates that the observed statistic is unusual under the null hypothesis of random distribution of features.\n",
    "5. Interpretation\n",
    "Draw Conclusions: Based on where your observed statistics fall in relation to the distributions from the shuffled data, draw conclusions about whether the features in each cluster are significantly different from what would be expected by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
