{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "import html\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and merge dataframes\n",
    "comments = pd.concat([pd.read_pickle('./pickle_dataframes/comments1.pkl'),\n",
    "                      pd.read_pickle('./pickle_dataframes/comments2.pkl')]).reset_index(drop=True)\n",
    "\n",
    "posts = pd.concat([pd.read_pickle('./pickle_dataframes/posts1.pkl'),\n",
    "                   pd.read_pickle('./pickle_dataframes/posts2.pkl'),\n",
    "                   pd.read_pickle('./pickle_dataframes/posts3.pkl')]).reset_index(drop=True)\n",
    "\n",
    "users = pd.read_pickle('./pickle_dataframes/users.pkl')\n",
    "postlinks = pd.read_pickle('./pickle_dataframes/posts_links.pkl')\n",
    "tags = pd.read_pickle('./pickle_dataframes/tags.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments = comments.sample(frac=0.1, random_state=0)\n",
    "#posts = posts.sample(frac=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From running various tests we found that the topic modelling method that yielded the best highest coherence score and the lowest perplexity score was:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify preprocess_text function\n",
    "def preprocess_text(text, remove_stopwords=False, use_lemmatize=True):\n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text.lower())\n",
    "\n",
    "    words = text.split()\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "    if use_lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define apply_lda_and_log function with run_name parameter\n",
    "def apply_topic_modeling_and_log(df, remove_stopwords, use_lemmatize, tags_weighting, run_name, ngram_range=(1, 1), max_features=1000):\n",
    "\n",
    "    # Initialize dictionaries to store topic distributions\n",
    "    lda_distributions = {}\n",
    "    nmf_distributions = {}\n",
    "\n",
    "    # Preprocess Title, Body, and Tags\n",
    "    df['Title'] = df['Title'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize))\n",
    "    df['Body'] = df['Body'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize))\n",
    "    df['Tags'] = df['Tags'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize))\n",
    "\n",
    "\n",
    "    # Combine Title, Body, and Tags with specified weight for Tags\n",
    "    # We Keep the original order (title, body, tags) as it reflects the natural flow of information\n",
    "    df['CombinedText'] = df['Title'] + ' ' + df['Body'] + ' ' + (df['Tags'] * tags_weighting)\n",
    "\n",
    "    # Create a Dictionary and Corpus needed for Topic Modeling\n",
    "    words = [doc.split() for doc in df['CombinedText']]\n",
    "    id2word = corpora.Dictionary(words)\n",
    "    corpus = [id2word.doc2bow(text) for text in words]\n",
    "\n",
    "    # Apply TF-IDF with the specified max_features\n",
    "    # ngram_range=(1, 2) for bi-grams, (1, 3) for tri-grams, and (2, 2) for only bi-grams\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['CombinedText'])\n",
    "\n",
    "    # Apply LDA and NMF for different numbers of topics\n",
    "    # Prepare a structured dictionary to store results with n_topics as part of the key\n",
    "    all_topics_results = {}\n",
    "    for n_topics in [5, 10, 15, 20]:\n",
    "        \n",
    "        # LDA\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "        lda.fit(tfidf_matrix)\n",
    "\n",
    "        # Extract Topic Distributions for LDA\n",
    "        lda_topic_distributions = lda.transform(tfidf_matrix)\n",
    "\n",
    "        # Normalize LDA Topic Distributions\n",
    "        lda_normalized = np.array(lda_topic_distributions) / np.sum(lda_topic_distributions, axis=1)[:, None]\n",
    "\n",
    "        # Calculate Coherence Score\n",
    "        lda_gensim = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, random_state=0)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_gensim, texts=words, dictionary=id2word, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "        # Calculate LDA Perplexity\n",
    "        lda_perplexity = lda.perplexity(tfidf_matrix)\n",
    "\n",
    "        # Extract and log the top words for each topic as a table\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        top_words_data = []\n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        # NMF\n",
    "        nmf_model = NMF(n_components=n_topics, random_state=0)\n",
    "        nmf_W = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # Normalize NMF Topic Distributions (nmf_W is already the topic distribution matrix)\n",
    "        nmf_normalized = np.array(nmf_W) / np.sum(nmf_W, axis=1)[:, None]\n",
    "\n",
    "        nmf_H = nmf_model.components_\n",
    "\n",
    "        # Calculate NMF Reconstruction Error\n",
    "        nmf_reconstruction_error = np.linalg.norm(tfidf_matrix - nmf_W.dot(nmf_H))\n",
    "\n",
    "        # Log the top words for each topic for NMF\n",
    "        nmf_top_words_data = []\n",
    "        for topic_idx, topic in enumerate(nmf_H):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            nmf_top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "\n",
    "        # Store the results including perplexity and reconstruction error\n",
    "        all_topics_results[f\"{run_name}_n_topics_{n_topics}\"] = {\n",
    "            'lda_normalized': lda_normalized,\n",
    "            'nmf_normalized': nmf_normalized,\n",
    "            'lda_coherence': coherence_lda,\n",
    "            'lda_perplexity': lda_perplexity,\n",
    "            'nmf_reconstruction_error': nmf_reconstruction_error,\n",
    "            'lda_top_words': top_words_data,\n",
    "            'nmf_top_words': nmf_top_words_data\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "    # Return the topic distributions\n",
    "    return all_topics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run_remove_True_lemmatize_True_weight_1_ngram_(1, 1)_maxfeat_1000\n",
      "Run_remove_True_lemmatize_True_weight_1_ngram_(1, 2)_maxfeat_1000\n",
      "Run_remove_True_lemmatize_True_weight_1_ngram_(1, 3)_maxfeat_1000\n",
      "Run_remove_True_lemmatize_True_weight_2_ngram_(1, 1)_maxfeat_1000\n",
      "Run_remove_True_lemmatize_True_weight_2_ngram_(1, 2)_maxfeat_1000\n",
      "Run_remove_True_lemmatize_True_weight_2_ngram_(1, 3)_maxfeat_1000\n",
      "Run_remove_True_lemmatize_True_weight_5_ngram_(1, 1)_maxfeat_1000\n",
      "Run_remove_True_lemmatize_True_weight_5_ngram_(1, 2)_maxfeat_1000\n",
      "Run_remove_True_lemmatize_True_weight_5_ngram_(1, 3)_maxfeat_1000\n"
     ]
    }
   ],
   "source": [
    "# Test various combinations\n",
    "use_lemmatize_options = [True]\n",
    "tags_weighting_options = [1, 2, 5]\n",
    "ngram_range_options = [(1, 1), (1, 2), (1, 3)]\n",
    "max_features_options = [1000]\n",
    "remove_stopwords = True \n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for use_lemmatize, tags_weighting, ngram_range, max_features in itertools.product(use_lemmatize_options, tags_weighting_options, ngram_range_options, max_features_options):\n",
    "    run_name = f\"Run_remove_{remove_stopwords}_lemmatize_{use_lemmatize}_weight_{tags_weighting}_ngram_{ngram_range}_maxfeat_{max_features}\"\n",
    "\n",
    "    topics_results = apply_topic_modeling_and_log(\n",
    "        posts,\n",
    "        remove_stopwords,\n",
    "        use_lemmatize,\n",
    "        tags_weighting, \n",
    "        run_name, \n",
    "        ngram_range, \n",
    "        max_features\n",
    "    )\n",
    "    print(run_name)\n",
    "    all_results.update(topics_results)\n",
    "\n",
    "# save dictionary to person_data.pkl file\n",
    "#with open('all_results.pkl', 'wb') as fp:\n",
    "#    pickle.dump(all_results, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best topic modelling technique + parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 LDA:\n",
      "Parameters: Run_remove_True_lemmatize_True_weight_1_ngram_(1, 2)_maxfeat_1000_n_topics_20, Coherence: 0.49817471503946614, Perplexity: 1718.5727994660563\n",
      "Parameters: Run_remove_True_lemmatize_True_weight_2_ngram_(1, 3)_maxfeat_1000_n_topics_20, Coherence: 0.4828223717706496, Perplexity: 1705.171976733514\n",
      "Parameters: Run_remove_True_lemmatize_True_weight_2_ngram_(1, 2)_maxfeat_1000_n_topics_20, Coherence: 0.4828223717706496, Perplexity: 1705.171976733514\n",
      "Parameters: Run_remove_True_lemmatize_True_weight_2_ngram_(1, 1)_maxfeat_1000_n_topics_20, Coherence: 0.4828223717706496, Perplexity: 1722.258531185024\n",
      "Parameters: Run_remove_True_lemmatize_True_weight_1_ngram_(1, 3)_maxfeat_1000_n_topics_15, Coherence: 0.481533622150989, Perplexity: 1546.6154196909786\n",
      "\n",
      "Top 5 NMF:\n",
      "Parameters: Run_remove_True_lemmatize_True_weight_5_ngram_(1, 3)_maxfeat_1000_n_topics_20, Reconstruction Error: 204.03639907100012\n",
      "Parameters: Run_remove_True_lemmatize_True_weight_5_ngram_(1, 2)_maxfeat_1000_n_topics_20, Reconstruction Error: 204.59141446491074\n",
      "Parameters: Run_remove_True_lemmatize_True_weight_5_ngram_(1, 1)_maxfeat_1000_n_topics_20, Reconstruction Error: 206.000088463304\n",
      "Parameters: Run_remove_True_lemmatize_True_weight_5_ngram_(1, 3)_maxfeat_1000_n_topics_15, Reconstruction Error: 206.5849670811247\n",
      "Parameters: Run_remove_True_lemmatize_True_weight_5_ngram_(1, 2)_maxfeat_1000_n_topics_15, Reconstruction Error: 207.14266759136262\n"
     ]
    }
   ],
   "source": [
    "all_results = pd.read_pickle('./all_results.pkl')\n",
    "\n",
    "import heapq\n",
    "\n",
    "# Initialize min-heaps to track the top 5 best scores and parameters for LDA and NMF\n",
    "top_5_lda = []\n",
    "top_5_nmf = []\n",
    "\n",
    "# Iterate through all results\n",
    "for run_name, results in all_results.items():\n",
    "    # Extract LDA and NMF scores\n",
    "    lda_score = (results['lda_coherence'], -results['lda_perplexity'])  # Negative perplexity for min-heap\n",
    "    nmf_score = -results['nmf_reconstruction_error']  # Negative error for min-heap\n",
    "\n",
    "    # Update top 5 LDA\n",
    "    if len(top_5_lda) < 5 or lda_score > top_5_lda[0][0]:\n",
    "        if len(top_5_lda) == 5:\n",
    "            heapq.heappop(top_5_lda)\n",
    "        heapq.heappush(top_5_lda, (lda_score, run_name))\n",
    "\n",
    "    # Update top 5 NMF\n",
    "    if len(top_5_nmf) < 5 or nmf_score > top_5_nmf[0][0]:\n",
    "        if len(top_5_nmf) == 5:\n",
    "            heapq.heappop(top_5_nmf)\n",
    "        heapq.heappush(top_5_nmf, (nmf_score, run_name))\n",
    "\n",
    "# Output top 5 LDA\n",
    "print(\"Top 5 LDA:\")\n",
    "for score, params in sorted(top_5_lda, reverse=True):\n",
    "    print(f\"Parameters: {params}, Coherence: {score[0]}, Perplexity: {-score[1]}\")\n",
    "\n",
    "# Output top 5 NMF\n",
    "print(\"\\nTop 5 NMF:\")\n",
    "for score, params in sorted(top_5_nmf, reverse=True):\n",
    "    print(f\"Parameters: {params}, Reconstruction Error: {-score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the best topic model\n",
    "- Parameters: Run_remove_True_lemmatize_True_weight_2_ngram_(1, 3)_maxfeat_1000_n_topics_20, Coherence: 0.4828223717706496, Perplexity: 1705.171976733514\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = posts[posts.PostTypeId==1]\n",
    "answers = posts[posts['PostTypeId'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_31810/1943339904.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  questions['Title'] = questions['Title'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize))\n",
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_31810/1943339904.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  questions['Body'] = questions['Body'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize))\n",
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_31810/1943339904.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  questions['Tags'] = questions['Tags'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize))\n",
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_31810/1943339904.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  questions['CombinedText'] = questions['Title'] + ' ' + questions['Body'] + ' ' + (questions['Tags'] * tags_weighting)\n"
     ]
    }
   ],
   "source": [
    "remove_stopwords = True\n",
    "use_lemmatize = True \n",
    "tags_weighting = 1\n",
    "ngram_range = (1, 2)\n",
    "max_features = 1000\n",
    "n_topics = 20\n",
    "\n",
    "# Apply preprocessing to each column\n",
    "questions['Title'] = questions['Title'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize))\n",
    "questions['Body'] = questions['Body'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize))\n",
    "questions['Tags'] = questions['Tags'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize))\n",
    "\n",
    "# Combine Title, Body, and Tags\n",
    "questions['CombinedText'] = questions['Title'] + ' ' + questions['Body'] + ' ' + (questions['Tags'] * tags_weighting)\n",
    "\n",
    "\n",
    "# Apply TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(questions['CombinedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_31810/766013537.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  questions['Topic'] = topic_assignments.argmax(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Apply TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(questions['CombinedText'])\n",
    "\n",
    "# Apply LDA\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Assign topics to questions\n",
    "topic_assignments = lda.transform(tfidf_matrix)\n",
    "questions['Topic'] = topic_assignments.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = questions[['Id', 'Topic']]\n",
    "\n",
    "# Merge to assign topics from questions to their answers\n",
    "answers_with_topics = answers.merge(topics_df, left_on='ParentId', right_on='Id', how='left')\n",
    "\n",
    "# Rename the 'Topic' column to something like 'InheritedTopic' to avoid confusion\n",
    "# answers_with_topics.rename(columns={'Topic': 'InheritedTopic'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Topic to every post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the posts dataframe with topic assignments\n",
    "#questions.to_pickle('./pickle_dataframes/questions_with_topics.pkl')\n",
    "#answers_with_topics.to_pickle('./pickle_dataframes/answers_with_topics.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id_x</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>Id_y</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2012-12-04 21:58:11.187</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;p&gt;First-past-the-post voting tends to result ...</td>\n",
       "      <td>26</td>\n",
       "      <td>2012-12-04 21:58:11.187</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2012-12-04 21:58:39.037</td>\n",
       "      <td>47</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;p&gt;Simple plurality voting has very little in ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2012-12-04 22:04:42.767</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>2012-12-04 22:17:48.290</td>\n",
       "      <td>85</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;p&gt;The standard terms of left and right politi...</td>\n",
       "      <td>18</td>\n",
       "      <td>2012-12-04 22:17:48.290</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>2012-12-04 22:26:45.633</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;p&gt;In an IRV (&lt;a href=\"https://en.wikipedia.or...</td>\n",
       "      <td>18</td>\n",
       "      <td>2017-06-14 10:16:02.733</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2012-12-04 22:29:16.460</td>\n",
       "      <td>40</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;p&gt;The mathematical phenomenon you're talking ...</td>\n",
       "      <td>26</td>\n",
       "      <td>2012-12-04 22:29:16.460</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085</th>\n",
       "      <td>81116</td>\n",
       "      <td>2</td>\n",
       "      <td>81095</td>\n",
       "      <td>-1</td>\n",
       "      <td>2023-09-02 12:14:00.063</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;blockquote&gt;&amp;#xA;&lt;p&gt;since the government is ra...</td>\n",
       "      <td>26824</td>\n",
       "      <td>2023-09-02 12:14:00.063</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>81095</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36086</th>\n",
       "      <td>81118</td>\n",
       "      <td>2</td>\n",
       "      <td>81095</td>\n",
       "      <td>-1</td>\n",
       "      <td>2023-09-02 16:28:41.700</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;p&gt;Does not the article you link to already an...</td>\n",
       "      <td>44212</td>\n",
       "      <td>2023-09-02 16:28:41.700</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>81095</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36087</th>\n",
       "      <td>81120</td>\n",
       "      <td>2</td>\n",
       "      <td>81119</td>\n",
       "      <td>-1</td>\n",
       "      <td>2023-09-02 21:05:10.440</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;p&gt;Generally speaking there's a large incumben...</td>\n",
       "      <td>18373</td>\n",
       "      <td>2023-09-02 22:24:01.417</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>81119</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36088</th>\n",
       "      <td>81124</td>\n",
       "      <td>2</td>\n",
       "      <td>81119</td>\n",
       "      <td>-1</td>\n",
       "      <td>2023-09-03 03:32:57.620</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;p&gt;Because they know where all the bodies are ...</td>\n",
       "      <td>39779</td>\n",
       "      <td>2023-09-03 03:32:57.620</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>81119</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36089</th>\n",
       "      <td>81125</td>\n",
       "      <td>2</td>\n",
       "      <td>81119</td>\n",
       "      <td>-1</td>\n",
       "      <td>2023-09-03 04:27:06.580</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;p&gt;This is almost entirely due to the party sy...</td>\n",
       "      <td>30035</td>\n",
       "      <td>2023-09-03 04:27:06.580</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>Comment: N/A</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>81119</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36090 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id_x  PostTypeId  ParentId  AcceptedAnswerId            CreationDate  \\\n",
       "0          4           2         1                -1 2012-12-04 21:58:11.187   \n",
       "1          5           2         1                -1 2012-12-04 21:58:39.037   \n",
       "2         13           2         7                -1 2012-12-04 22:17:48.290   \n",
       "3         18           2         9                -1 2012-12-04 22:26:45.633   \n",
       "4         19           2         2                -1 2012-12-04 22:29:16.460   \n",
       "...      ...         ...       ...               ...                     ...   \n",
       "36085  81116           2     81095                -1 2023-09-02 12:14:00.063   \n",
       "36086  81118           2     81095                -1 2023-09-02 16:28:41.700   \n",
       "36087  81120           2     81119                -1 2023-09-02 21:05:10.440   \n",
       "36088  81124           2     81119                -1 2023-09-03 03:32:57.620   \n",
       "36089  81125           2     81119                -1 2023-09-03 04:27:06.580   \n",
       "\n",
       "       Score  ViewCount                                               Body  \\\n",
       "0          7         -1  <p>First-past-the-post voting tends to result ...   \n",
       "1         47         -1  <p>Simple plurality voting has very little in ...   \n",
       "2         85         -1  <p>The standard terms of left and right politi...   \n",
       "3         12         -1  <p>In an IRV (<a href=\"https://en.wikipedia.or...   \n",
       "4         40         -1  <p>The mathematical phenomenon you're talking ...   \n",
       "...      ...        ...                                                ...   \n",
       "36085     -1         -1  <blockquote>&#xA;<p>since the government is ra...   \n",
       "36086      0         -1  <p>Does not the article you link to already an...   \n",
       "36087      4         -1  <p>Generally speaking there's a large incumben...   \n",
       "36088     -1         -1  <p>Because they know where all the bodies are ...   \n",
       "36089      0         -1  <p>This is almost entirely due to the party sy...   \n",
       "\n",
       "       OwnerUserId        LastActivityDate         Title          Tags  \\\n",
       "0               26 2012-12-04 21:58:11.187  Comment: N/A  Comment: N/A   \n",
       "1                8 2012-12-04 22:04:42.767  Comment: N/A  Comment: N/A   \n",
       "2               18 2012-12-04 22:17:48.290  Comment: N/A  Comment: N/A   \n",
       "3               18 2017-06-14 10:16:02.733  Comment: N/A  Comment: N/A   \n",
       "4               26 2012-12-04 22:29:16.460  Comment: N/A  Comment: N/A   \n",
       "...            ...                     ...           ...           ...   \n",
       "36085        26824 2023-09-02 12:14:00.063  Comment: N/A  Comment: N/A   \n",
       "36086        44212 2023-09-02 16:28:41.700  Comment: N/A  Comment: N/A   \n",
       "36087        18373 2023-09-02 22:24:01.417  Comment: N/A  Comment: N/A   \n",
       "36088        39779 2023-09-03 03:32:57.620  Comment: N/A  Comment: N/A   \n",
       "36089        30035 2023-09-03 04:27:06.580  Comment: N/A  Comment: N/A   \n",
       "\n",
       "       AnswerCount  CommentCount   Id_y  Topic  \n",
       "0               -1             1      1     18  \n",
       "1               -1             1      1     18  \n",
       "2               -1             7      7      7  \n",
       "3               -1             2      9     18  \n",
       "4               -1            10      2     18  \n",
       "...            ...           ...    ...    ...  \n",
       "36085           -1             0  81095     17  \n",
       "36086           -1             1  81095     17  \n",
       "36087           -1             2  81119     18  \n",
       "36088           -1             0  81119     18  \n",
       "36089           -1             0  81119     18  \n",
       "\n",
       "[36090 rows x 16 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_with_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
