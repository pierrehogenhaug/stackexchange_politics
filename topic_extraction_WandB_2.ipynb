{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "import html\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df_comments1 = pd.read_pickle('./pickle_dataframes/comments1.pkl')\n",
    "df_comments2 = pd.read_pickle('./pickle_dataframes/comments2.pkl')\n",
    "df_comments = pd.concat([df_comments1,df_comments2])\n",
    "df_comments.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_posts1 = pd.read_pickle('./pickle_dataframes/posts1.pkl')\n",
    "df_posts2 = pd.read_pickle('./pickle_dataframes/posts2.pkl')\n",
    "df_posts3 = pd.read_pickle('./pickle_dataframes/posts3.pkl')\n",
    "df_posts = pd.concat([df_posts1, df_posts2, df_posts3])\n",
    "df_posts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_postlinks = pd.read_pickle('./pickle_dataframes/posts_links.pkl')\n",
    "df_tags = pd.read_pickle('./pickle_dataframes/tags.pkl')\n",
    "df_users = pd.read_pickle('./pickle_dataframes/users.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at our DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df = df_posts[df_posts['PostTypeId'] == 1]\n",
    "\n",
    "# questions_sample_df = questions_df.sample(frac=0.25)\n",
    "questions_df = questions_df.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify preprocess_text function\n",
    "def preprocess_text(text, remove_stopwords=False, use_lemmatize=True, use_stemmer=False):\n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text.lower())\n",
    "\n",
    "    words = text.split()\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "    if use_lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    elif use_stemmer:  # Apply stemming only if use_stemmer is True\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WandB Timeeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define apply_lda_and_log function with run_name parameter\n",
    "def apply_lda_and_log(df, remove_stopwords, use_lemmatize, use_stemmer, tags_weighting, run_name):\n",
    "    # Start a new WandB run with the specified name\n",
    "    wandb.init(project=\"stackexchange_politics\", name=run_name)\n",
    "    \n",
    "    # Preprocess Title, Body, and Tags\n",
    "    df['Title'] = df['Title'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "    df['Body'] = df['Body'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "    df['Tags'] = df['Tags'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "\n",
    "\n",
    "    # Combine Title, Body, and Tags with specified weight for Tags\n",
    "    # We Keep the original order (title, body, tags) as it reflects the natural flow of information\n",
    "    df['CombinedText'] = df['Title'] + ' ' + df['Body'] + ' ' + (df['Tags'] * tags_weighting)\n",
    "\n",
    "    # Create a Dictionary and Corpus needed for Topic Modeling\n",
    "    words = [doc.split() for doc in df['CombinedText']]\n",
    "    id2word = corpora.Dictionary(words)\n",
    "    corpus = [id2word.doc2bow(text) for text in words]\n",
    "\n",
    "    # Apply TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['CombinedText'])\n",
    "\n",
    "    # Apply LDA for different numbers of topics\n",
    "    for n_topics in [5, 10, 15, 20]:\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "        lda.fit(tfidf_matrix)\n",
    "\n",
    "        # Calculate Coherence Score\n",
    "        lda_gensim = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, random_state=0)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_gensim, texts=words, dictionary=id2word, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "        # Log Coherence and Perplexity Score\n",
    "        wandb.log({\"coherence_score\": coherence_lda, \"perplexity_score\": lda.perplexity(tfidf_matrix)})\n",
    "        \n",
    "        # Extract and log the top words for each topic as a table\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        top_words_data = []\n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        # Create a WandB Table with top words data\n",
    "        columns = [\"Topic\"] + [f\"Word {i+1}\" for i in range(10)]\n",
    "        top_words_table = wandb.Table(data=top_words_data, columns=columns)\n",
    "        \n",
    "        # Log the table to WandB\n",
    "        wandb.log({f\"n_topics_{n_topics}_cleaned_{str(remove_stopwords)}_lemmatize_{str(use_lemmatize)}_weight_{tags_weighting}\": top_words_table})\n",
    "\n",
    "    \n",
    "    # Close WandB run\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define apply_lda_and_log function with run_name parameter\n",
    "def apply_topic_modeling_and_log(df, remove_stopwords, use_lemmatize, use_stemmer, tags_weighting, run_name):\n",
    "    # Start a new WandB run with the specified name\n",
    "    wandb.init(project=\"stackexchange_politics\", entity=\"s223730\", name=run_name)\n",
    "    # Make sure the script runs in the correct WandB project\n",
    "    print(wandb.run.project_name())\n",
    "\n",
    "    # Preprocess Title, Body, and Tags\n",
    "    df['Title'] = df['Title'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "    df['Body'] = df['Body'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "    df['Tags'] = df['Tags'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "\n",
    "\n",
    "    # Combine Title, Body, and Tags with specified weight for Tags\n",
    "    # We Keep the original order (title, body, tags) as it reflects the natural flow of information\n",
    "    df['CombinedText'] = df['Title'] + ' ' + df['Body'] + ' ' + (df['Tags'] * tags_weighting)\n",
    "\n",
    "    # Create a Dictionary and Corpus needed for Topic Modeling\n",
    "    words = [doc.split() for doc in df['CombinedText']]\n",
    "    id2word = corpora.Dictionary(words)\n",
    "    corpus = [id2word.doc2bow(text) for text in words]\n",
    "\n",
    "    # Apply TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['CombinedText'])\n",
    "\n",
    "    # Apply LDA and NMF for different numbers of topics\n",
    "    for n_topics in [5, 10, 15, 20]:\n",
    "        \n",
    "        # LDA\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "        lda.fit(tfidf_matrix)\n",
    "\n",
    "        # Calculate Coherence Score\n",
    "        lda_gensim = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, random_state=0)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_gensim, texts=words, dictionary=id2word, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "        # Log Coherence and Perplexity Score\n",
    "        wandb.log({\"coherence_score\": coherence_lda, \"perplexity_score\": lda.perplexity(tfidf_matrix)})\n",
    "        \n",
    "        # Extract and log the top words for each topic as a table\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        top_words_data = []\n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        # Create a WandB Table with top words data\n",
    "        columns = [\"Topic\"] + [f\"Word {i+1}\" for i in range(10)]\n",
    "        top_words_table = wandb.Table(data=top_words_data, columns=columns)\n",
    "        \n",
    "        # Log the table to WandB\n",
    "        wandb.log({f\"n_topics_{n_topics}_cleaned_{str(remove_stopwords)}_lemmatize_{str(use_lemmatize)}_weight_{tags_weighting}\": top_words_table})\n",
    "\n",
    "        # NMF\n",
    "        nmf_model = NMF(n_components=n_topics, random_state=0)\n",
    "        nmf_W = nmf_model.fit_transform(tfidf_matrix)\n",
    "        nmf_H = nmf_model.components_\n",
    "\n",
    "        # Log the top words for each topic for NMF\n",
    "        nmf_top_words_data = []\n",
    "        for topic_idx, topic in enumerate(nmf_H):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            nmf_top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        nmf_top_words_table = wandb.Table(data=nmf_top_words_data, columns=columns)\n",
    "        wandb.log({f\"nmf_n_topics_{n_topics}\": nmf_top_words_table})\n",
    "\n",
    "    \n",
    "    # Close WandB run\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define apply_lda_and_log function with run_name parameter\n",
    "def apply_topic_modeling_and_log(df, remove_stopwords, use_lemmatize, use_stemmer, tags_weighting, run_name, ngram_range=(1, 1)):\n",
    "    # Start a new WandB run with the specified name\n",
    "    wandb.init(project=\"stackexchange_politics\", entity=\"s223730\", name=run_name)\n",
    "    # Make sure the script runs in the correct WandB project\n",
    "    print(wandb.run.project_name())\n",
    "\n",
    "    # Preprocess Title, Body, and Tags\n",
    "    df['Title'] = df['Title'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "    df['Body'] = df['Body'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "    df['Tags'] = df['Tags'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "\n",
    "\n",
    "    # Combine Title, Body, and Tags with specified weight for Tags\n",
    "    # We Keep the original order (title, body, tags) as it reflects the natural flow of information\n",
    "    df['CombinedText'] = df['Title'] + ' ' + df['Body'] + ' ' + (df['Tags'] * tags_weighting)\n",
    "\n",
    "    # Create a Dictionary and Corpus needed for Topic Modeling\n",
    "    words = [doc.split() for doc in df['CombinedText']]\n",
    "    id2word = corpora.Dictionary(words)\n",
    "    corpus = [id2word.doc2bow(text) for text in words]\n",
    "\n",
    "    # Apply TF-IDF\n",
    "    # ngram_range=(1, 2) for bi-grams, (1, 3) for tri-grams, and (2, 2) for only bi-grams\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=ngram_range)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['CombinedText'])\n",
    "\n",
    "    # Apply LDA and NMF for different numbers of topics\n",
    "    for n_topics in [5, 10, 15, 20]:\n",
    "        \n",
    "        # LDA\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "        lda.fit(tfidf_matrix)\n",
    "\n",
    "        # Calculate Coherence Score\n",
    "        lda_gensim = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, random_state=0)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_gensim, texts=words, dictionary=id2word, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "        # Log Coherence and Perplexity Score\n",
    "        wandb.log({\"coherence_score\": coherence_lda, \"perplexity_score\": lda.perplexity(tfidf_matrix)})\n",
    "        \n",
    "        # Extract and log the top words for each topic as a table\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        top_words_data = []\n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        # Create a WandB Table with top words data\n",
    "        columns = [\"Topic\"] + [f\"Word {i+1}\" for i in range(10)]\n",
    "        top_words_table = wandb.Table(data=top_words_data, columns=columns)\n",
    "        \n",
    "        # Log the table to WandB\n",
    "        wandb.log({f\"n_topics_{n_topics}_cleaned_{str(remove_stopwords)}_lemmatize_{str(use_lemmatize)}_weight_{tags_weighting}\": top_words_table})\n",
    "\n",
    "        # NMF\n",
    "        nmf_model = NMF(n_components=n_topics, random_state=0)\n",
    "        nmf_W = nmf_model.fit_transform(tfidf_matrix)\n",
    "        nmf_H = nmf_model.components_\n",
    "\n",
    "        # Log the top words for each topic for NMF\n",
    "        nmf_top_words_data = []\n",
    "        for topic_idx, topic in enumerate(nmf_H):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            nmf_top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        nmf_top_words_table = wandb.Table(data=nmf_top_words_data, columns=columns)\n",
    "        wandb.log({f\"nmf_n_topics_{n_topics}\": nmf_top_words_table})\n",
    "\n",
    "    # Close WandB run\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define apply_lda_and_log function with run_name parameter\n",
    "def apply_topic_modeling_and_log__(df, remove_stopwords, use_lemmatize, use_stemmer, tags_weighting, run_name, ngram_range=(1, 1), max_features=1000):\n",
    "    # Start a new WandB run with the specified name\n",
    "    wandb.init(project=\"stackexchange_politics\", entity=\"s223730\", name=run_name)\n",
    "\n",
    "    # Preprocess Title, Body, and Tags\n",
    "    df['Title'] = df['Title'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "    df['Body'] = df['Body'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "    df['Tags'] = df['Tags'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "\n",
    "\n",
    "    # Combine Title, Body, and Tags with specified weight for Tags\n",
    "    # We Keep the original order (title, body, tags) as it reflects the natural flow of information\n",
    "    df['CombinedText'] = df['Title'] + ' ' + df['Body'] + ' ' + (df['Tags'] * tags_weighting)\n",
    "\n",
    "    # Create a Dictionary and Corpus needed for Topic Modeling\n",
    "    words = [doc.split() for doc in df['CombinedText']]\n",
    "    id2word = corpora.Dictionary(words)\n",
    "    corpus = [id2word.doc2bow(text) for text in words]\n",
    "\n",
    "    # Apply TF-IDF with the specified max_features\n",
    "    # ngram_range=(1, 2) for bi-grams, (1, 3) for tri-grams, and (2, 2) for only bi-grams\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['CombinedText'])\n",
    "\n",
    "    # Apply LDA and NMF for different numbers of topics\n",
    "    for n_topics in [5, 10, 15, 20]:\n",
    "        \n",
    "        # LDA\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "        lda.fit(tfidf_matrix)\n",
    "\n",
    "        # Extract Topic Distributions for LDA\n",
    "        lda_topic_distributions = lda.transform(tfidf_matrix)\n",
    "\n",
    "        # Normalize LDA Topic Distributions\n",
    "        lda_normalized = np.array(lda_topic_distributions) / np.sum(lda_topic_distributions, axis=1)[:, None]\n",
    "\n",
    "        # Log LDA normalized distributions\n",
    "        wandb.log({\"lda_normalized_distributions\": wandb.Table(data=lda_normalized.tolist())})\n",
    "\n",
    "        # Calculate Coherence Score\n",
    "        lda_gensim = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, random_state=0)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_gensim, texts=words, dictionary=id2word, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "        # Log Coherence and Perplexity Score\n",
    "        wandb.log({\"coherence_score\": coherence_lda, \"perplexity_score\": lda.perplexity(tfidf_matrix)})\n",
    "        \n",
    "        # Extract and log the top words for each topic as a table\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        top_words_data = []\n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        # Create a WandB Table with top words data\n",
    "        columns = [\"Topic\"] + [f\"Word {i+1}\" for i in range(10)]\n",
    "        top_words_table = wandb.Table(data=top_words_data, columns=columns)\n",
    "        \n",
    "        # Log the table to WandB\n",
    "        wandb.log({f\"n_topics_{n_topics}_cleaned_{str(remove_stopwords)}_lemmatize_{str(use_lemmatize)}_weight_{tags_weighting}\": top_words_table})\n",
    "\n",
    "        # NMF\n",
    "        nmf_model = NMF(n_components=n_topics, random_state=0)\n",
    "        nmf_W = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # Normalize NMF Topic Distributions (nmf_W is already the topic distribution matrix)\n",
    "        nmf_normalized = np.array(nmf_W) / np.sum(nmf_W, axis=1)[:, None]\n",
    "\n",
    "        # Log NMF normalized distributions\n",
    "        wandb.log({\"nmf_normalized_distributions\": wandb.Table(data=nmf_normalized.tolist())})\n",
    "\n",
    "        nmf_H = nmf_model.components_\n",
    "\n",
    "        # Log the top words for each topic for NMF\n",
    "        nmf_top_words_data = []\n",
    "        for topic_idx, topic in enumerate(nmf_H):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            nmf_top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        nmf_top_words_table = wandb.Table(data=nmf_top_words_data, columns=columns)\n",
    "        wandb.log({f\"nmf_n_topics_{n_topics}\": nmf_top_words_table})\n",
    "\n",
    "    # Close WandB run\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Coherence and Perplexity\n",
    "\n",
    "- **Coherence**: This measures how well the topics are defined. A higher coherence score generally indicates that the topics are more interpretable and distinct. Look for configurations with the highest coherence scores.\n",
    "\n",
    "- **Perplexity**: This is a measure of how well the model predicts a sample. In general, lower perplexity indicates a better model. However, perplexity can sometimes be misleading, especially if the model is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locality Sensitive Hashing (LSH)\n",
    "- **Application**: LSH is typically used for similarity searches in high-dimensional data. In your case, it can help identify documents (posts or comments) that are similar in terms of their topic distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running different LDA configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                             remove_stopwords=False, \n",
    "                             use_lemmatize=False, \n",
    "                             use_stemmer=False,\n",
    "                             tags_weighting=1, \n",
    "                             run_name=\"MaxFeatures_500\",\n",
    "                             ngram_range=(1, 1),\n",
    "                             max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                             remove_stopwords=False, \n",
    "                             use_lemmatize=False, \n",
    "                             use_stemmer=False,\n",
    "                             tags_weighting=1, \n",
    "                             run_name=\"MaxFeatures_1000\",\n",
    "                             ngram_range=(1, 1),\n",
    "                             max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                             remove_stopwords=False, \n",
    "                             use_lemmatize=False, \n",
    "                             use_stemmer=False,\n",
    "                             tags_weighting=1, \n",
    "                             run_name=\"MaxFeatures_2000\",\n",
    "                             ngram_range=(1, 1),\n",
    "                             max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline removed stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords tags weight = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_TagsWeight2\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_TagsWeight2\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_TagsWeight2\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords tags weight = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_TagsWeight5\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_TagsWeight5\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=1000)                  \n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_TagsWeight5\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Lemmatized\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Lemmatized\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Lemmatized\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, lemmatized tags weight = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Lemmatized_TagsWeight2\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Lemmatized_TagsWeight2\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Lemmatized_TagsWeight2\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, lemmatized tags weight = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Lemmatized_TagsWeight5\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Lemmatized_TagsWeight5\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Lemmatized_TagsWeight5\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Stemmed\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Stemmed\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Stemmed\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, stemmed tags weight = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Stemmed_TagsWeight2\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Stemmed_TagsWeight2\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Stemmed_TagsWeight2\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Removed stopwords, stemmed tags weight = 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Stemmed_TagsWeight5\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Stemmed_TagsWeight5\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Stemmed_TagsWeight5\",\n",
    "                  ngram_range=(1, 1),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same but with Unigrams and Bigrams (1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline removed stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Bigram\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Bigram\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Bigram\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords tags weight = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Bigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Bigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Bigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords tags weight = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Bigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Bigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Bigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Lemmatized_Bigram\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Lemmatized_Bigram\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Lemmatized_Bigram\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, lemmatized tags weight = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Lemmatized_Bigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Lemmatized_Bigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Lemmatized_Bigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, lemmatized tags weight = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Lemmatized_Bigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Lemmatized_Bigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Lemmatized_Bigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Stemmed_Bigram\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Stemmed_Bigram\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Stemmed_Bigram\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, stemmed tags weight = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Stemmed_Bigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Stemmed_Bigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Stemmed_Bigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, stemmed tags weight = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Stemmed_Bigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Stemmed_Bigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Stemmed_Bigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 2),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same but with Unigrams, Bigrams, and Trigrams (1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline removed stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Trigram\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Trigram\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Trigram\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords tags weight = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Trigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Trigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Trigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords tags weight = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Trigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Trigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Trigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Lemmatized_Trigram\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Lemmatized_Trigram\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Lemmatized_Trigram\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, lemmatized tags weight = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Lemmatized_Trigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Lemmatized_Trigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Lemmatized_Trigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, lemmatized tags weight = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Lemmatized_Trigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Lemmatized_Trigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=True, \n",
    "                  use_stemmer=False,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Lemmatized_Trigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Stemmed_Trigram\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Stemmed_Trigram\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=1, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Stemmed_Trigram\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, stemmed tags weight = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Stemmed_Trigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Stemmed_Trigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=2, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Stemmed_Trigram_TagsWeight2\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed stopwords, stemmed tags weight = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_500_StopwordsRemoved_Stemmed_Trigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=500)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_1000_StopwordsRemoved_Stemmed_Trigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=1000)\n",
    "\n",
    "apply_topic_modeling_and_log(df_posts[df_posts['PostTypeId'] == 1], \n",
    "                  remove_stopwords=True, \n",
    "                  use_lemmatize=False, \n",
    "                  use_stemmer=True,\n",
    "                  tags_weighting=5, \n",
    "                  run_name=\"MaxFeatures_2000_StopwordsRemoved_Stemmed_Trigram_TagsWeight5\",\n",
    "                  ngram_range=(1, 3),\n",
    "                  max_features=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestions for Improvements\n",
    "- Adjust StopWords?\n",
    "- **Hyperparameter Tuning**: Tune the parameters of the LDA model,\n",
    "    - learning decay\n",
    "    - batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Should only Post-Level have topic assigned to them?\n",
    "    - Then Sub-Posts are assigned the same topic as Post\n",
    "    - Comments are assigned the same topic as Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df.to_pickle('questions_cleaned_text.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "Apply Sentiment Analysis on:\n",
    "- Post Level\n",
    "- Sub Post Level\n",
    "- Comment Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User-Post-Topic Matrix**: \n",
    "- Create a matrix where rows represent users and columns represent topics. \n",
    "- Each cell contains the count of posts/comments a user has made in a particular topic.\n",
    "    - Post Level: where `PostTypeId` == 1 AND `ParentId` == -1\n",
    "    - Sub Post Level: where `PostTypeId` == 1 AND `ParentId` != -1\n",
    "    - Comment Level: where `PostTypeId` == 2\n",
    "- **Include Post Statistics**\n",
    "    - AcceptedAnswerId\n",
    "    - Score\n",
    "    - ViewCount\n",
    "    - AnswerCount\n",
    "    - CommentCount\n",
    "- **Include Comment Statistics**\n",
    "    - Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering Algorithms**\n",
    "- K-Means: Use the user-topic matrix to cluster users. Determine the optimal number of clusters (communities) using the Elbow method or Silhouette score.\n",
    "\n",
    "- Hierarchical Clustering: Useful for understanding the data structure and forming hierarchical communities. Dendrograms can visualize the community structure.\n",
    "\n",
    "- DBSCAN: Good for datasets with noise and clusters of varying densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Market Basket Analysis**\n",
    "- Association Rules and Apriori Algorithm: \n",
    "    - Treat each user's set of topics as a 'basket'. \n",
    "    - Identify strong rules where the presence of one topic implies the presence of another in a user's posts\n",
    "    - This can highlight topic-based communities.\n",
    "- Frequent Itemsets: \n",
    "    - Identify sets of topics that frequently occur together in users' posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Locality Sensitive Hashing (LSH)**\n",
    "- LSH for Dimension Reduction: \n",
    "    - If the user-topic matrix is very sparse and high-dimensional, LSH can reduce dimensions while preserving the similarity structure. This can make subsequent clustering more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced Techniques**\n",
    "- PCY Algorithm: If you're dealing with very large data, this algorithm efficiently finds frequent itemsets, useful in subsequent association rule mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Davies-Bouldin Index**: Evaluate the quality of clusters. \n",
    "- Lower Davies-Bouldin index values signify better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
