{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "import html\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df_comments1 = pd.read_pickle('./pickle_dataframes/comments1.pkl')\n",
    "df_comments2 = pd.read_pickle('./pickle_dataframes/comments2.pkl')\n",
    "df_comments = pd.concat([df_comments1,df_comments2])\n",
    "df_comments.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_posts1 = pd.read_pickle('./pickle_dataframes/posts1.pkl')\n",
    "df_posts2 = pd.read_pickle('./pickle_dataframes/posts2.pkl')\n",
    "df_posts3 = pd.read_pickle('./pickle_dataframes/posts3.pkl')\n",
    "df_posts = pd.concat([df_posts1, df_posts2, df_posts3])\n",
    "df_posts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_postlinks = pd.read_pickle('./pickle_dataframes/posts_links.pkl')\n",
    "df_tags = pd.read_pickle('./pickle_dataframes/tags.pkl')\n",
    "df_users = pd.read_pickle('./pickle_dataframes/users.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df = df_posts[df_posts['PostTypeId'] == 1]\n",
    "sub_posts = df_posts[df_posts['PostTypeId'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic model: only posts from active users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove entries with -1 in UserId and OwnerUserId columns\n",
    "df_comments = df_comments[df_comments['UserId'] != -1]\n",
    "df_posts = df_posts[df_posts['OwnerUserId'] != -1]\n",
    "\n",
    "# Calculate post and comment counts for each user\n",
    "user_posts_count = df_posts.groupby('OwnerUserId')['OwnerUserId'].size().rename('PostCount')\n",
    "user_comments_count = df_comments.groupby('UserId')['UserId'].size().rename('CommentCount')\n",
    "\n",
    "# Merge counts with user data and fill missing values\n",
    "user_data = (df_users\n",
    "             .merge(user_posts_count, left_on='Id', right_index=True, how='left')\n",
    "             .merge(user_comments_count, left_on='Id', right_index=True, how='left')\n",
    "             .fillna({'PostCount': 0, 'CommentCount': 0}))\n",
    "\n",
    "# Add a column for total activity and filter for active users\n",
    "active_users = user_data.assign(TotalActivity=lambda x: x['PostCount'] + x['CommentCount'])\n",
    "active_users = active_users[active_users['TotalActivity'] > 20]\n",
    "\n",
    "# Set of active user IDs\n",
    "active_user_ids = set(active_users['Id'])\n",
    "\n",
    "# Filter dataframes for active user activity\n",
    "filtered_questions_df = questions_df[questions_df['OwnerUserId'].isin(active_user_ids)]\n",
    "filtered_sub_posts = sub_posts[(sub_posts['ParentId'].isin(active_user_ids)) |\n",
    "                                (sub_posts['Id'].isin(active_user_ids))]\n",
    "\n",
    "# Combine filters for comments related to active users\n",
    "active_user_post_ids = set(df_posts[df_posts['OwnerUserId'].isin(active_user_ids)]['Id'])\n",
    "filtered_comments = df_comments[(df_comments['UserId'].isin(active_user_ids)) | \n",
    "                                (df_comments['PostId'].isin(active_user_post_ids))].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224028\n",
      "139902 \n",
      "\n",
      "505260\n",
      "13342 \n",
      "\n",
      "1054752\n",
      "998796 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(questions_df.size)\n",
    "print(filtered_questions_df.size, '\\n')\n",
    "\n",
    "print(sub_posts.size)\n",
    "print(filtered_sub_posts.size, '\\n')\n",
    "\n",
    "print(df_comments.size)\n",
    "print(filtered_comments.size, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify preprocess_text function\n",
    "def preprocess_text(text, remove_stopwords=False, use_lemmatize=True, use_stemmer=False):\n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text.lower())\n",
    "\n",
    "    words = text.split()\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "    if use_lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    elif use_stemmer:  # Apply stemming only if use_stemmer is True\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB Timeeee\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define apply_lda_and_log function with run_name parameter\n",
    "def apply_topic_modeling_and_log(df, remove_stopwords, use_lemmatize, use_stemmer, tags_weighting, run_name, ngram_range=(1, 1), max_features=1000):\n",
    "    # Start a new WandB run with the specified name\n",
    "    wandb.init(project=\"stackexchange_politics\", entity=\"s223730\", name=run_name)\n",
    "\n",
    "    # Initialize dictionaries to store topic distributions\n",
    "    lda_distributions = {}\n",
    "    nmf_distributions = {}\n",
    "\n",
    "    # Preprocess Title, Body, and Tags\n",
    "    df['Title'] = df['Title'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "    df['Body'] = df['Body'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "    df['Tags'] = df['Tags'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "\n",
    "\n",
    "    # Combine Title, Body, and Tags with specified weight for Tags\n",
    "    # We Keep the original order (title, body, tags) as it reflects the natural flow of information\n",
    "    df['CombinedText'] = df['Title'] + ' ' + df['Body'] + ' ' + (df['Tags'] * tags_weighting)\n",
    "\n",
    "    # Create a Dictionary and Corpus needed for Topic Modeling\n",
    "    words = [doc.split() for doc in df['CombinedText']]\n",
    "    id2word = corpora.Dictionary(words)\n",
    "    corpus = [id2word.doc2bow(text) for text in words]\n",
    "\n",
    "    # Apply TF-IDF with the specified max_features\n",
    "    # ngram_range=(1, 2) for bi-grams, (1, 3) for tri-grams, and (2, 2) for only bi-grams\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['CombinedText'])\n",
    "\n",
    "    # Apply LDA and NMF for different numbers of topics\n",
    "    # Prepare a structured dictionary to store results with n_topics as part of the key\n",
    "    all_topics_results = {}\n",
    "    for n_topics in [5, 10, 15, 20]:\n",
    "        \n",
    "        # LDA\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "        lda.fit(tfidf_matrix)\n",
    "\n",
    "        # Extract Topic Distributions for LDA\n",
    "        lda_topic_distributions = lda.transform(tfidf_matrix)\n",
    "\n",
    "        # Normalize LDA Topic Distributions\n",
    "        lda_normalized = np.array(lda_topic_distributions) / np.sum(lda_topic_distributions, axis=1)[:, None]\n",
    "\n",
    "        # Calculate Coherence Score\n",
    "        lda_gensim = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, random_state=0)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_gensim, texts=words, dictionary=id2word, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "        # Calculate LDA Perplexity\n",
    "        lda_perplexity = lda.perplexity(tfidf_matrix)\n",
    "\n",
    "        # Log Coherence and Perplexity Score\n",
    "        wandb.log({\"coherence_score\": coherence_lda, \"perplexity_score\": lda.perplexity(tfidf_matrix)})\n",
    "        \n",
    "        # Extract and log the top words for each topic as a table\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        top_words_data = []\n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        # Create a WandB Table with top words data\n",
    "        columns = [\"Topic\"] + [f\"Word {i+1}\" for i in range(10)]\n",
    "        top_words_table = wandb.Table(data=top_words_data, columns=columns)\n",
    "        \n",
    "        # Log the table to WandB\n",
    "        wandb.log({f\"n_topics_{n_topics}_cleaned_{str(remove_stopwords)}_lemmatize_{str(use_lemmatize)}_weight_{tags_weighting}\": top_words_table})\n",
    "\n",
    "        # NMF\n",
    "        nmf_model = NMF(n_components=n_topics, random_state=0)\n",
    "        nmf_W = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # Normalize NMF Topic Distributions (nmf_W is already the topic distribution matrix)\n",
    "        nmf_normalized = np.array(nmf_W) / np.sum(nmf_W, axis=1)[:, None]\n",
    "\n",
    "        nmf_H = nmf_model.components_\n",
    "\n",
    "        # Calculate NMF Reconstruction Error\n",
    "        nmf_reconstruction_error = np.linalg.norm(tfidf_matrix - nmf_W.dot(nmf_H))\n",
    "\n",
    "        # Log the top words for each topic for NMF\n",
    "        nmf_top_words_data = []\n",
    "        for topic_idx, topic in enumerate(nmf_H):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            nmf_top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        nmf_top_words_table = wandb.Table(data=nmf_top_words_data, columns=columns)\n",
    "        wandb.log({f\"nmf_n_topics_{n_topics}\": nmf_top_words_table})\n",
    "\n",
    "        # Store the results including perplexity and reconstruction error\n",
    "        all_topics_results[f\"{run_name}_n_topics_{n_topics}\"] = {\n",
    "            'lda_normalized': lda_normalized,\n",
    "            'nmf_normalized': nmf_normalized,\n",
    "            'lda_coherence': coherence_lda,\n",
    "            'lda_perplexity': lda_perplexity,\n",
    "            'nmf_reconstruction_error': nmf_reconstruction_error,\n",
    "            'lda_top_words': top_words_data,\n",
    "            'nmf_top_words': nmf_top_words_data\n",
    "        }\n",
    "        \n",
    "    # Close WandB run\n",
    "    wandb.finish()\n",
    "\n",
    "    # Return the topic distributions\n",
    "    return all_topics_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running different LDA configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your parameter ranges\n",
    "remove_stopwords = True\n",
    "use_lemmatize_options = [False, True]\n",
    "use_stemmer_options = [False, True]\n",
    "tags_weighting_options = [1, 2, 5]\n",
    "ngram_range_options = [(1, 1), (1, 2), (1, 3)]\n",
    "max_features_options = [1000, 2000]\n",
    "\n",
    "# Store the results for each n_topics uniquely\n",
    "all_results = {}\n",
    "\n",
    "# Iterate over the combinations of other options\n",
    "for use_lemmatize, use_stemmer, tags_weighting, ngram_range, max_features in itertools.product(use_lemmatize_options, use_stemmer_options, tags_weighting_options, ngram_range_options, max_features_options):\n",
    "    \n",
    "    # Skip the iteration if both lemmatize and stemmer are set to True\n",
    "    if use_lemmatize and use_stemmer:\n",
    "        continue\n",
    "    \n",
    "    # Construct a unique run name for this combination\n",
    "    run_name = f\"Run_remove_{remove_stopwords}_lemmatize_{use_lemmatize}_stemmer_{use_stemmer}_weight_{tags_weighting}_ngram_{ngram_range}_maxfeat_{max_features}\"\n",
    "\n",
    "    # Run the function and get the results\n",
    "    topics_results = apply_topic_modeling_and_log(\n",
    "        questions_df, \n",
    "        remove_stopwords, \n",
    "        use_lemmatize, \n",
    "        use_stemmer, \n",
    "        tags_weighting, \n",
    "        run_name, \n",
    "        ngram_range, \n",
    "        max_features\n",
    "    )\n",
    "\n",
    "    # Update all_results to include these structured results\n",
    "    all_results.update(topics_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a file or handle them as needed\n",
    "# For example, saving to a pickle file\n",
    "import pickle\n",
    "with open('topic_modeling_results_n_topics.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestions for Improvements\n",
    "- Adjust StopWords?\n",
    "- **Hyperparameter Tuning**: Tune the parameters of the LDA model,\n",
    "    - learning decay\n",
    "    - batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Should only Post-Level have topic assigned to them?\n",
    "    - Then Sub-Posts are assigned the same topic as Post\n",
    "    - Comments are assigned the same topic as Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df.to_pickle('questions_cleaned_text.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "Apply Sentiment Analysis on:\n",
    "- Post Level\n",
    "- Sub Post Level\n",
    "- Comment Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User-Post-Topic Matrix**: \n",
    "- Create a matrix where rows represent users and columns represent topics. \n",
    "- Each cell contains the count of posts/comments a user has made in a particular topic.\n",
    "    - Post Level: where `PostTypeId` == 1 AND `ParentId` == -1\n",
    "    - Sub Post Level: where `PostTypeId` == 1 AND `ParentId` != -1\n",
    "    - Comment Level: where `PostTypeId` == 2\n",
    "- **Include Post Statistics**\n",
    "    - AcceptedAnswerId\n",
    "    - Score\n",
    "    - ViewCount\n",
    "    - AnswerCount\n",
    "    - CommentCount\n",
    "- **Include Comment Statistics**\n",
    "    - Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering Algorithms**\n",
    "- K-Means: Use the user-topic matrix to cluster users. Determine the optimal number of clusters (communities) using the Elbow method or Silhouette score.\n",
    "\n",
    "- Hierarchical Clustering: Useful for understanding the data structure and forming hierarchical communities. Dendrograms can visualize the community structure.\n",
    "\n",
    "- DBSCAN: Good for datasets with noise and clusters of varying densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Market Basket Analysis**\n",
    "- Association Rules and Apriori Algorithm: \n",
    "    - Treat each user's set of topics as a 'basket'. \n",
    "    - Identify strong rules where the presence of one topic implies the presence of another in a user's posts\n",
    "    - This can highlight topic-based communities.\n",
    "- Frequent Itemsets: \n",
    "    - Identify sets of topics that frequently occur together in users' posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Locality Sensitive Hashing (LSH)**\n",
    "- LSH for Dimension Reduction: \n",
    "    - If the user-topic matrix is very sparse and high-dimensional, LSH can reduce dimensions while preserving the similarity structure. This can make subsequent clustering more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced Techniques**\n",
    "- PCY Algorithm: If you're dealing with very large data, this algorithm efficiently finds frequent itemsets, useful in subsequent association rule mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Davies-Bouldin Index**: Evaluate the quality of clusters. \n",
    "- Lower Davies-Bouldin index values signify better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
