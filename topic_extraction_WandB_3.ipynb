{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "import html\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df_comments1 = pd.read_pickle('./pickle_dataframes/comments1.pkl')\n",
    "df_comments2 = pd.read_pickle('./pickle_dataframes/comments2.pkl')\n",
    "df_comments = pd.concat([df_comments1,df_comments2])\n",
    "df_comments.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_posts1 = pd.read_pickle('./pickle_dataframes/posts1.pkl')\n",
    "df_posts2 = pd.read_pickle('./pickle_dataframes/posts2.pkl')\n",
    "df_posts3 = pd.read_pickle('./pickle_dataframes/posts3.pkl')\n",
    "df_posts = pd.concat([df_posts1, df_posts2, df_posts3])\n",
    "df_posts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_postlinks = pd.read_pickle('./pickle_dataframes/posts_links.pkl')\n",
    "df_tags = pd.read_pickle('./pickle_dataframes/tags.pkl')\n",
    "df_users = pd.read_pickle('./pickle_dataframes/users.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at our DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PostId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>UserId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Is it fair to inquire about the disadvantages ...</td>\n",
       "      <td>2012-12-04 22:00:00.933</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>I could have reformulated the question, but at...</td>\n",
       "      <td>2012-12-04 22:02:37.737</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Source on this? I don't see how it could possi...</td>\n",
       "      <td>2012-12-04 22:10:10.070</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@Nick122 In a parliamentary system like the No...</td>\n",
       "      <td>2012-12-04 22:14:33.463</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes, but you will give a negative vote by voti...</td>\n",
       "      <td>2012-12-04 22:16:29.437</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184667</th>\n",
       "      <td>351991</td>\n",
       "      <td>81122</td>\n",
       "      <td>0</td>\n",
       "      <td>Meh, not my DV, but this is going to be tough ...</td>\n",
       "      <td>2023-09-03 04:09:05.587</td>\n",
       "      <td>18373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184668</th>\n",
       "      <td>351992</td>\n",
       "      <td>38746</td>\n",
       "      <td>0</td>\n",
       "      <td>The mistake you made is assuming good faith by...</td>\n",
       "      <td>2023-09-03 04:09:13.657</td>\n",
       "      <td>21362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184669</th>\n",
       "      <td>351993</td>\n",
       "      <td>81121</td>\n",
       "      <td>0</td>\n",
       "      <td>Because most others might not have such funds?...</td>\n",
       "      <td>2023-09-03 04:12:33.097</td>\n",
       "      <td>18373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184670</th>\n",
       "      <td>351994</td>\n",
       "      <td>81024</td>\n",
       "      <td>0</td>\n",
       "      <td>@Steve There has been some per capita GDP prog...</td>\n",
       "      <td>2023-09-03 04:14:38.957</td>\n",
       "      <td>8016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184671</th>\n",
       "      <td>351995</td>\n",
       "      <td>81118</td>\n",
       "      <td>0</td>\n",
       "      <td>Indeed, one might insert the Kennedy quote abo...</td>\n",
       "      <td>2023-09-03 04:33:10.497</td>\n",
       "      <td>18373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184672 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id  PostId  Score  \\\n",
       "0            1       1      9   \n",
       "1            3       1      3   \n",
       "2            7       2      2   \n",
       "3           13       2      1   \n",
       "4           15       2      0   \n",
       "...        ...     ...    ...   \n",
       "184667  351991   81122      0   \n",
       "184668  351992   38746      0   \n",
       "184669  351993   81121      0   \n",
       "184670  351994   81024      0   \n",
       "184671  351995   81118      0   \n",
       "\n",
       "                                                     Text  \\\n",
       "0       Is it fair to inquire about the disadvantages ...   \n",
       "1       I could have reformulated the question, but at...   \n",
       "2       Source on this? I don't see how it could possi...   \n",
       "3       @Nick122 In a parliamentary system like the No...   \n",
       "4       Yes, but you will give a negative vote by voti...   \n",
       "...                                                   ...   \n",
       "184667  Meh, not my DV, but this is going to be tough ...   \n",
       "184668  The mistake you made is assuming good faith by...   \n",
       "184669  Because most others might not have such funds?...   \n",
       "184670  @Steve There has been some per capita GDP prog...   \n",
       "184671  Indeed, one might insert the Kennedy quote abo...   \n",
       "\n",
       "                  CreationDate  UserId  \n",
       "0      2012-12-04 22:00:00.933      28  \n",
       "1      2012-12-04 22:02:37.737      18  \n",
       "2      2012-12-04 22:10:10.070      45  \n",
       "3      2012-12-04 22:14:33.463      43  \n",
       "4      2012-12-04 22:16:29.437      45  \n",
       "...                        ...     ...  \n",
       "184667 2023-09-03 04:09:05.587   18373  \n",
       "184668 2023-09-03 04:09:13.657   21362  \n",
       "184669 2023-09-03 04:12:33.097   18373  \n",
       "184670 2023-09-03 04:14:38.957    8016  \n",
       "184671 2023-09-03 04:33:10.497   18373  \n",
       "\n",
       "[184672 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36090, 14)\n",
      "(36090, 14)\n"
     ]
    }
   ],
   "source": [
    "questions_df = df_posts[df_posts['PostTypeId'] == 1]\n",
    "\n",
    "sub_posts_1 = df_posts[(df_posts['PostTypeId'] == 2) &\n",
    "                        (df_posts['ParentId']!=-1)]\n",
    "\n",
    "\n",
    "sub_posts = df_posts[df_posts['PostTypeId'] == 2]\n",
    "\n",
    "print(sub_posts.shape)\n",
    "print(sub_posts_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify preprocess_text function\n",
    "def preprocess_text(text, remove_stopwords=False, use_lemmatize=True, use_stemmer=False):\n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text.lower())\n",
    "\n",
    "    words = text.split()\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "    if use_lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    elif use_stemmer:  # Apply stemming only if use_stemmer is True\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WandB Timeeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define apply_lda_and_log function with run_name parameter\n",
    "def apply_topic_modeling_and_log(df, remove_stopwords, use_lemmatize, use_stemmer, tags_weighting, run_name, ngram_range=(1, 1), max_features=1000):\n",
    "    # Start a new WandB run with the specified name\n",
    "    wandb.init(project=\"stackexchange_politics\", entity=\"s223730\", name=run_name)\n",
    "\n",
    "    # Initialize dictionaries to store topic distributions\n",
    "    lda_distributions = {}\n",
    "    nmf_distributions = {}\n",
    "\n",
    "    # Preprocess Title, Body, and Tags\n",
    "    df['Title'] = df['Title'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "    df['Body'] = df['Body'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "    df['Tags'] = df['Tags'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "\n",
    "\n",
    "    # Combine Title, Body, and Tags with specified weight for Tags\n",
    "    # We Keep the original order (title, body, tags) as it reflects the natural flow of information\n",
    "    df['CombinedText'] = df['Title'] + ' ' + df['Body'] + ' ' + (df['Tags'] * tags_weighting)\n",
    "\n",
    "    # Create a Dictionary and Corpus needed for Topic Modeling\n",
    "    words = [doc.split() for doc in df['CombinedText']]\n",
    "    id2word = corpora.Dictionary(words)\n",
    "    corpus = [id2word.doc2bow(text) for text in words]\n",
    "\n",
    "    # Apply TF-IDF with the specified max_features\n",
    "    # ngram_range=(1, 2) for bi-grams, (1, 3) for tri-grams, and (2, 2) for only bi-grams\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['CombinedText'])\n",
    "\n",
    "    # Apply LDA and NMF for different numbers of topics\n",
    "    # Prepare a structured dictionary to store results with n_topics as part of the key\n",
    "    all_topics_results = {}\n",
    "    for n_topics in [5, 10, 15, 20]:\n",
    "        \n",
    "        # LDA\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "        lda.fit(tfidf_matrix)\n",
    "\n",
    "        # Extract Topic Distributions for LDA\n",
    "        lda_topic_distributions = lda.transform(tfidf_matrix)\n",
    "\n",
    "        # Normalize LDA Topic Distributions\n",
    "        lda_normalized = np.array(lda_topic_distributions) / np.sum(lda_topic_distributions, axis=1)[:, None]\n",
    "\n",
    "        # Calculate Coherence Score\n",
    "        lda_gensim = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, random_state=0)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_gensim, texts=words, dictionary=id2word, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "        # Log Coherence and Perplexity Score\n",
    "        wandb.log({\"coherence_score\": coherence_lda, \"perplexity_score\": lda.perplexity(tfidf_matrix)})\n",
    "        \n",
    "        # Extract and log the top words for each topic as a table\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        top_words_data = []\n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        # Create a WandB Table with top words data\n",
    "        columns = [\"Topic\"] + [f\"Word {i+1}\" for i in range(10)]\n",
    "        top_words_table = wandb.Table(data=top_words_data, columns=columns)\n",
    "        \n",
    "        # Log the table to WandB\n",
    "        wandb.log({f\"n_topics_{n_topics}_cleaned_{str(remove_stopwords)}_lemmatize_{str(use_lemmatize)}_weight_{tags_weighting}\": top_words_table})\n",
    "\n",
    "        # NMF\n",
    "        nmf_model = NMF(n_components=n_topics, random_state=0)\n",
    "        nmf_W = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # Normalize NMF Topic Distributions (nmf_W is already the topic distribution matrix)\n",
    "        nmf_normalized = np.array(nmf_W) / np.sum(nmf_W, axis=1)[:, None]\n",
    "\n",
    "        nmf_H = nmf_model.components_\n",
    "\n",
    "        # Log the top words for each topic for NMF\n",
    "        nmf_top_words_data = []\n",
    "        for topic_idx, topic in enumerate(nmf_H):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            nmf_top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        nmf_top_words_table = wandb.Table(data=nmf_top_words_data, columns=columns)\n",
    "        wandb.log({f\"nmf_n_topics_{n_topics}\": nmf_top_words_table})\n",
    "\n",
    "        # Store the results for each n_topics\n",
    "        all_topics_results[f\"{run_name}_n_topics_{n_topics}\"] = {\n",
    "            'lda_normalized': lda_normalized,\n",
    "            'nmf_normalized': nmf_normalized,\n",
    "            'lda_coherence': coherence_lda,\n",
    "            'lda_top_words': top_words_data,\n",
    "            'nmf_top_words': nmf_top_words_data\n",
    "        }\n",
    "        \n",
    "    # Close WandB run\n",
    "    wandb.finish()\n",
    "\n",
    "    # Return the topic distributions\n",
    "    return all_topics_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running different LDA configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/phog/Desktop/cleverThings/socialGraphs23/politics/stackexchange_politics/wandb/run-20231121_141724-tqgcd2zy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/s223730/stackexchange_politics/runs/tqgcd2zy' target=\"_blank\">Run_remove_True_lemmatize_False_stemmer_False_weight_1_ngram_(1, 1)_maxfeat_1000</a></strong> to <a href='https://wandb.ai/s223730/stackexchange_politics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/s223730/stackexchange_politics' target=\"_blank\">https://wandb.ai/s223730/stackexchange_politics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/s223730/stackexchange_politics/runs/tqgcd2zy' target=\"_blank\">https://wandb.ai/s223730/stackexchange_politics/runs/tqgcd2zy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_12271/1384959364.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Title'] = df['Title'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_12271/1384959364.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Body'] = df['Body'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_12271/1384959364.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Tags'] = df['Tags'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_12271/1384959364.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['CombinedText'] = df['Title'] + ' ' + df['Body'] + ' ' + (df['Tags'] * tags_weighting)\n",
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_12271/1384959364.py:72: RuntimeWarning: invalid value encountered in divide\n",
      "  nmf_normalized = np.array(nmf_W) / np.sum(nmf_W, axis=1)[:, None]\n",
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_12271/1384959364.py:72: RuntimeWarning: invalid value encountered in divide\n",
      "  nmf_normalized = np.array(nmf_W) / np.sum(nmf_W, axis=1)[:, None]\n",
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_12271/1384959364.py:72: RuntimeWarning: invalid value encountered in divide\n",
      "  nmf_normalized = np.array(nmf_W) / np.sum(nmf_W, axis=1)[:, None]\n",
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_12271/1384959364.py:72: RuntimeWarning: invalid value encountered in divide\n",
      "  nmf_normalized = np.array(nmf_W) / np.sum(nmf_W, axis=1)[:, None]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c170f9dc79a44ebea5a77654c4b1f3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.035 MB of 0.035 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>coherence_score</td><td>█▁▂▁</td></tr><tr><td>perplexity_score</td><td>▁▄▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>coherence_score</td><td>0.33588</td></tr><tr><td>perplexity_score</td><td>2650.51427</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Run_remove_True_lemmatize_False_stemmer_False_weight_1_ngram_(1, 1)_maxfeat_1000</strong> at: <a href='https://wandb.ai/s223730/stackexchange_politics/runs/tqgcd2zy' target=\"_blank\">https://wandb.ai/s223730/stackexchange_politics/runs/tqgcd2zy</a><br/>Synced 5 W&B file(s), 8 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231121_141724-tqgcd2zy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1431bb4188d44255b30de8c11a6238c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011169000000356593, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/phog/Desktop/cleverThings/socialGraphs23/politics/stackexchange_politics/wandb/run-20231121_142039-k3b5rd8w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/s223730/stackexchange_politics/runs/k3b5rd8w' target=\"_blank\">Run_remove_True_lemmatize_False_stemmer_False_weight_1_ngram_(1, 1)_maxfeat_2000</a></strong> to <a href='https://wandb.ai/s223730/stackexchange_politics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/s223730/stackexchange_politics' target=\"_blank\">https://wandb.ai/s223730/stackexchange_politics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/s223730/stackexchange_politics/runs/k3b5rd8w' target=\"_blank\">https://wandb.ai/s223730/stackexchange_politics/runs/k3b5rd8w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_12271/1384959364.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Title'] = df['Title'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_12271/1384959364.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Body'] = df['Body'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_12271/1384959364.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Tags'] = df['Tags'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_12271/1384959364.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['CombinedText'] = df['Title'] + ' ' + df['Body'] + ' ' + (df['Tags'] * tags_weighting)\n"
     ]
    }
   ],
   "source": [
    "# Define your parameter ranges\n",
    "remove_stopwords = True\n",
    "use_lemmatize_options = [False, True]\n",
    "use_stemmer_options = [False, True]\n",
    "tags_weighting_options = [1, 2, 5]\n",
    "ngram_range_options = [(1, 1), (1, 2), (1, 3)]\n",
    "max_features_options = [1000, 2000]\n",
    "\n",
    "# Store the results for each n_topics uniquely\n",
    "all_results = {}\n",
    "\n",
    "# Iterate over the combinations of other options\n",
    "for use_lemmatize, use_stemmer, tags_weighting, ngram_range, max_features in itertools.product(use_lemmatize_options, use_stemmer_options, tags_weighting_options, ngram_range_options, max_features_options):\n",
    "    \n",
    "    # Construct a unique run name for this combination\n",
    "    run_name = f\"Run_remove_{remove_stopwords}_lemmatize_{use_lemmatize}_stemmer_{use_stemmer}_weight_{tags_weighting}_ngram_{ngram_range}_maxfeat_{max_features}\"\n",
    "\n",
    "    # Run the function and get the results\n",
    "    topics_results = apply_topic_modeling_and_log(\n",
    "        questions_df, \n",
    "        remove_stopwords, \n",
    "        use_lemmatize, \n",
    "        use_stemmer, \n",
    "        tags_weighting, \n",
    "        run_name, \n",
    "        ngram_range, \n",
    "        max_features\n",
    "    )\n",
    "\n",
    "    # Update all_results to include these structured results\n",
    "    all_results.update(topics_results)\n",
    "\n",
    "# Save the results to a file or handle them as needed\n",
    "# For example, saving to a pickle file\n",
    "import pickle\n",
    "with open('topic_modeling_results_n_topics.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestions for Improvements\n",
    "- Adjust StopWords?\n",
    "- **Hyperparameter Tuning**: Tune the parameters of the LDA model,\n",
    "    - learning decay\n",
    "    - batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Should only Post-Level have topic assigned to them?\n",
    "    - Then Sub-Posts are assigned the same topic as Post\n",
    "    - Comments are assigned the same topic as Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df.to_pickle('questions_cleaned_text.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "Apply Sentiment Analysis on:\n",
    "- Post Level\n",
    "- Sub Post Level\n",
    "- Comment Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User-Post-Topic Matrix**: \n",
    "- Create a matrix where rows represent users and columns represent topics. \n",
    "- Each cell contains the count of posts/comments a user has made in a particular topic.\n",
    "    - Post Level: where `PostTypeId` == 1 AND `ParentId` == -1\n",
    "    - Sub Post Level: where `PostTypeId` == 1 AND `ParentId` != -1\n",
    "    - Comment Level: where `PostTypeId` == 2\n",
    "- **Include Post Statistics**\n",
    "    - AcceptedAnswerId\n",
    "    - Score\n",
    "    - ViewCount\n",
    "    - AnswerCount\n",
    "    - CommentCount\n",
    "- **Include Comment Statistics**\n",
    "    - Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering Algorithms**\n",
    "- K-Means: Use the user-topic matrix to cluster users. Determine the optimal number of clusters (communities) using the Elbow method or Silhouette score.\n",
    "\n",
    "- Hierarchical Clustering: Useful for understanding the data structure and forming hierarchical communities. Dendrograms can visualize the community structure.\n",
    "\n",
    "- DBSCAN: Good for datasets with noise and clusters of varying densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Market Basket Analysis**\n",
    "- Association Rules and Apriori Algorithm: \n",
    "    - Treat each user's set of topics as a 'basket'. \n",
    "    - Identify strong rules where the presence of one topic implies the presence of another in a user's posts\n",
    "    - This can highlight topic-based communities.\n",
    "- Frequent Itemsets: \n",
    "    - Identify sets of topics that frequently occur together in users' posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Locality Sensitive Hashing (LSH)**\n",
    "- LSH for Dimension Reduction: \n",
    "    - If the user-topic matrix is very sparse and high-dimensional, LSH can reduce dimensions while preserving the similarity structure. This can make subsequent clustering more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced Techniques**\n",
    "- PCY Algorithm: If you're dealing with very large data, this algorithm efficiently finds frequent itemsets, useful in subsequent association rule mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Davies-Bouldin Index**: Evaluate the quality of clusters. \n",
    "- Lower Davies-Bouldin index values signify better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
